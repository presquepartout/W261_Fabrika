{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##DATSCIW261 ASSIGNMENT #5\n",
    "\n",
    "MIDS UC Berkeley, Machine Learning at Scale\n",
    "\n",
    "DATSCIW261 ASSIGNMENT #4\n",
    "\n",
    "Version 2016-02-18 (FINAL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###HW 5.0\n",
    "\n",
    "**What is a data warehouse?** \n",
    "\n",
    "A data warehouse is a central repository of integrated data used for reporting and analysis purposes. Typically the data is loaded into the warehouse from disparate sources, and the data undergoes preliminary processing to make it report-ready or analysis-ready. This process is referred to as \"ETL\" - Extract, Transform, and Load. \n",
    "\n",
    "An example of a data warehouse for a large company might be: data from sales, manufacturing, R&D, service, finance are all stored in the warehouse. The transform process might normalize data, one-hot encode data, and so on. \n",
    "\n",
    "**What is a Star schema? When is it used?**\n",
    "\n",
    "A Star Schema is a star-shaped denormalized database schema that consists of two types of tables: Fact tables and Dimension tables. Fact tables contain measurable, quantitative data and Dimension tables contain descriptive data. For example, a fact table for a toy store would contain units made on particular dates, units sold on dates, where units are distributed. Corresponding dimension tables would contain toy models, colors, sizes, and other characteristics of the toys. \n",
    "\n",
    "Typically Dimension tables have fewer records but more attributes than Fact tables. Fact tables lie in the center of the Star Schema, with associated Dimension tables branching out from the central Fact tables. \n",
    "\n",
    "Star Schemas are useful in cases where certain queries about transactions are made frequently. For example, suppose a business sells Products in a collection of Stores. There would be many queries of a particular type such as how many products sold in certain periods; how many sold by location. It would make sense to build a Star Schema with transactions as the Fact table, and Dimension tables such as Date, Store, and Product. The Dimension tables would be small but hold all particulars about each dimension. \n",
    "\n",
    "In a MapReduce context, Dimension Tables could be held in memory or read into a hash table and the far larger Fact table used for streaming input. \n",
    "\n",
    "If relationships become too complex, a Star Schema might not be as useful. Star Schemas, since they are not normalized, have a risk of becoming inconsistent. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###HW 5.1\n",
    "\n",
    "**In the database world What is 3NF?** \n",
    "3NF stands for third normal form. A set of database tables is normalized if redundant information is removed. This aids in database consistency and accuracy; if values are repeated, they can get out of sync. There are several levels of normalization. \n",
    "First normal form: a table is in first normal form if and only if the domain of each attribute contains only atomic (indivisible) values, and the value of each attribute contains only a single value from that domain.\n",
    "\n",
    "Second normal form: a table is in second normal form if it is in first normal form and no non-prime attribute is dependent on any proper subset of any candidate key of the table. A non-prime attribute of a table is an attribute that is not a part of any candidate key of the table. (A candidate key uniquely defines rows in the table). \n",
    "\n",
    "A table R is in 3NF if the following two conditions hold: \n",
    "\n",
    "- The relation R (table) is in second normal form (2NF)\n",
    "- Every non-prime attribute of R is non-transitively dependent on every key of R.\n",
    "\n",
    "3NF is a more rigorous normalization condition than first or second normal form. \n",
    "\n",
    "**Does machine learning use data in 3NF? If so why?**\n",
    "Machine learning learns from samples. The samples have characteristics that a model can learn. For machine learning it is best to represent data with all of its characteristics present. In 3NF, information can be spread out in different tables. \n",
    "\n",
    "**In what form does ML consume data?**\n",
    "ML consumes data as samples with collections of features. \n",
    "\n",
    "**Why would one use log files that are denormalized?**\n",
    "To assemble all the features that could influence a model together. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###HW 5.2\n",
    "\n",
    "Using MRJob, implement a hashside join (memory-backed map-side) for left, \n",
    "right and inner joins. Run your code on the  data used in HW 4.4: (Recall HW 4.4: Find the most frequent visitor of each page using mrjob and the output of 4.2  (i.e., transfromed log file). In this output please include the webpage URL, webpageID and Visitor ID.)\n",
    ":\n",
    "\n",
    "Justify which table you chose as the Left table in this hashside join.\n",
    "\n",
    "Please report the number of rows resulting from:\n",
    "\n",
    "(1) Left joining Table Left with Table Right\n",
    "(2) Right joining Table Left with Table Right\n",
    "(3) Inner joining Table Left with Table Right"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The two tables involved are: \n",
    "\n",
    "- a webpage URL table containing URLs and webpageID - composed of \"A, \" rows of the `anonymous-msweb.data` file. \n",
    "\n",
    "- a transaction table containing visited webpageID and visitorID for each visit. \n",
    "\n",
    "The first table is much smaller than the second and thus will be used as the Left table. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting format_URL_data.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile format_URL_data.py\n",
    "#!/usr/bin/python\n",
    "\"\"\"This program reformats the Microsoft anonymous visitor data.\n",
    "In particular, customer IDs and corresponding visited site IDs\n",
    "are placed on the same line. \n",
    "\"\"\"\n",
    "\n",
    "import sys\n",
    "\n",
    "checkV = False\n",
    "prestring = \"\"\n",
    "\n",
    "for line in sys.stdin:\n",
    "    \n",
    "    if line[:2] == \"I,\":\n",
    "        line = line.strip()\n",
    "        recI = line.split(\",\")\n",
    "        prestring = recI[2].strip('\"')\n",
    "        \n",
    "    elif line[:2] == \"A,\":\n",
    "        line = line.strip()\n",
    "        recA = line.split(\",\")\n",
    "        #A,1288,1,\"library\",\"/library\"\n",
    "        print \"%s,%s%s\" % (recA[1],prestring,recA[4].strip('\"'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!chmod a+x format_URL_data.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!cat ../HW4/anonymous-msweb.data | python format_URL_data.py > hw52input.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1287,www.microsoft.com/autoroute\r\n",
      "1288,www.microsoft.com/library\r\n",
      "1289,www.microsoft.com/masterchef\r\n",
      "1297,www.microsoft.com/centroam\r\n"
     ]
    }
   ],
   "source": [
    "!head -n4 hw52input.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing hw52A.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile hw52A.py\n",
    "\n",
    "from mrjob.job import MRJob, MRStep\n",
    "import mrjob\n",
    "import csv\n",
    "import sys\n",
    "\n",
    "def csv_readline(line):\n",
    "    \"\"\"Hashed left join.\"\"\"\n",
    "    return csv.reader([line]).next()\n",
    "\n",
    "class HashLeftJoin(MRJob):\n",
    "    \n",
    "    # Have to use Raw Protocol in order to get sorting (using 3rd key) to work.\n",
    "    INTERNAL_PROTOCOL = mrjob.protocol.RawProtocol\n",
    "    OUTPUT_PROTOCOL = mrjob.protocol.RawProtocol\n",
    "    \n",
    "    def mapper1(self, line_no, line):\n",
    "        cell = csv_readline(line)\n",
    "        # page,visitor \\t count\n",
    "        yield \",\".join([cell[1], cell[4]]), \"1\"\n",
    "        \n",
    "    def reducer1(self, key, values):\n",
    "        \"\"\"Sum up the visit count per (page, visitor) pair.\"\"\"\n",
    "        total = sum([int(v) for v in values])\n",
    "        fields = key.split(\",\")\n",
    "        # page \\t visitor \\t total\n",
    "        yield fields[0], \"\\t\".join([fields[1], str(total)])\n",
    "\n",
    "    def reducer2_init(self):\n",
    "        # Build the dictionary of pageId:url\n",
    "        self.urls = {}\n",
    "        with open(\"hw42output.txt\", \"r\") as f:\n",
    "            for fields in csv.reader(f):\n",
    "                self.urls[fields[0]] = fields[1]\n",
    "                \n",
    "    def reducer2(self, key, values):\n",
    "        # url \\t pageId \\t visitor \\t total\n",
    "        yield self.BASE_URL + self.urls[key] + \"\\t\" + key, values.next()\n",
    "    \n",
    "    def steps(self):\n",
    "        return [\n",
    "            MRStep(mapper=self.mapper1,\n",
    "                   reducer=self.reducer1,\n",
    "                  ),\n",
    "            MRStep(reducer_init=self.reducer2_init,\n",
    "                   reducer=self.reducer2,\n",
    "                   jobconf={\n",
    "                    \"stream.num.map.output.key.fields\":\"3\",\n",
    "                    \"mapreduce.job.output.key.comparator.class\":\n",
    "                        \"org.apache.hadoop.mapred.lib.KeyFieldBasedComparator\",\n",
    "                    \"mapreduce.partition.keycomparator.options\":\"-k3,3nr\"\n",
    "                          })]\n",
    "\n",
    "    \n",
    "if __name__ == '__main__':\n",
    "    HashLeftJoin()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!chmod a+x hw52A.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from hw52A import HashLeftJoin\n",
    "import csv\n",
    "\n",
    "mr_job = HashLeftJoin(args=['./hw42output.txt'])\n",
    "with mr_job.make_runner() as runner:\n",
    "    runner.run()\n",
    "    for line in runner.stream_output():\n",
    "        print mr_job.parse_output_line(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%writefile hw52B.py\n",
    "#!/usr/bin/python\n",
    "\"\"\"\n",
    "Hashed table right join. \n",
    "\"\"\"\n",
    "from mrjob.job import MRJob\n",
    "from mrjob.step import MRStep\n",
    "import csv\n",
    "\n",
    "def csv_readline(line):\n",
    "    \"\"\"Given a sting CSV line, return a list of strings.\"\"\"\n",
    "    for row in csv.reader([line]):\n",
    "        return row\n",
    "\n",
    "class TopWords52B(MRJob):\n",
    "    \"\"\"\n",
    "    This mrjob program performs in-memory hashed joins. \n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "        \n",
    "    def mapper_get_words(self, line_no, line):\n",
    "        \"\"\"This mapper connects read in lines with hashed URLs. \"\"\"\n",
    "        line = line.strip()\n",
    "        \n",
    "        rec = line.split(\"\\t\")\n",
    "        words = rec[0].split(\" \")\n",
    "        for word in words:\n",
    "            yield word,int(rec[1])\n",
    "                  \n",
    "\n",
    "    def reducer_count_words(self, word, counts):\n",
    "        \"\"\"xxx\"\"\"    \n",
    "        #print \"at first reducer\"\n",
    "        total = sum(i for i in counts)\n",
    "        yield (total, word), total \n",
    "            \n",
    "    def mapper_routekeys(self, word, total):\n",
    "        \"\"\"Sends key value pairs from Job 1 to hadoop shuffle for Job 2.\"\"\"\n",
    "        yield word, total\n",
    "    \n",
    "    def reducer_find10_init(self):\n",
    "        self.count = 50\n",
    "            \n",
    "    def reducer_find_10_max(self,keyt,total):\n",
    "        \"\"\"Outputs the top ten counts and words.\"\"\"    \n",
    "        if self.count > 0: \n",
    "            \n",
    "            yield keyt, total.next()\n",
    "            self.count -= 1\n",
    "            \n",
    "                  \n",
    "    def steps(self):\n",
    "        return [\n",
    "            MRStep(mapper=self.mapper_get_words,\n",
    "                   reducer=self.reducer_count_words),\n",
    "            MRStep(#mapper=self.mapper_routekeys,\n",
    "                   reducer_init=self.reducer_find10_init,\n",
    "                   reducer=self.reducer_find_10_max,\n",
    "                   jobconf={\n",
    "                    \"stream.num.map.output.key.fields\":\"2\",\n",
    "                    \"mapreduce.output.key.comparator.class\":\n",
    "                        \"org.apache.hadoop.mapred.lib.KeyFieldBasedComparator\",\n",
    "                    \"mapreduce.text.keycomparator.options\":\"-k1,1nr k2,2\"\n",
    "                          })\n",
    "        ]\n",
    "\n",
    "        \n",
    "if __name__ == '__main__':\n",
    "    TopWords52B.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###HW5.3.\n",
    "\n",
    "EDA of googlebooks 5-grams using MrJob and EMR. \n",
    "Part 1: longest 5-gram and word count. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting hw53A.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile hw53A.py\n",
    "#!/usr/bin/python\n",
    "\"\"\"\n",
    "Longest 5-gram.\n",
    "\"\"\"\n",
    "from mrjob.job import MRJob\n",
    "from mrjob.step import MRStep\n",
    "\n",
    "class TopWords53A(MRJob):\n",
    "\n",
    "    def mapper_get_words(self, _, line):\n",
    "        \"\"\"\n",
    "        Output line lengths and lines. \n",
    "        \"\"\"\n",
    "        line = line.strip()\n",
    "        rec = line.split(\"\\t\")\n",
    "        linelen = len(rec[0])\n",
    "        yield \"maxlen\",(linelen, rec[0])  \n",
    "        \n",
    "    def combiner_maxlen(self, word, pairs):\n",
    "        \"\"\"\n",
    "        Take the maximum line length. \n",
    "        \"\"\"\n",
    "        pl = list(pairs)\n",
    "        lens = [e[0] for e in pl]\n",
    "        maxidx = lens.index(max(lens))\n",
    "        yield \"maxlen\", (pl[maxidx])\n",
    "\n",
    "    def reducer_maxlen(self, word, pairs):\n",
    "        \"\"\"\n",
    "        Take the maximum line length. \n",
    "        \"\"\" \n",
    "        pl = list(pairs)\n",
    "        lens = [e[0] for e in pl]\n",
    "        maxidx = lens.index(max(lens))\n",
    "        yield pl[maxidx]\n",
    "\n",
    "            \n",
    "                  \n",
    "    def steps(self):\n",
    "        return [\n",
    "            MRStep(mapper=self.mapper_get_words,\n",
    "                   combiner=self.combiner_maxlen,\n",
    "                   reducer=self.reducer_maxlen)\n",
    "        ]\n",
    "     \n",
    "if __name__ == '__main__':\n",
    "    TopWords53A.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!chmod a+x hw53A.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:mrjob.runner:\n",
      "WARNING:mrjob.runner:PLEASE NOTE: Starting in mrjob v0.5.0, protocols will be strict by default. It's recommended you run your job with --strict-protocols or set up mrjob.conf as described at https://pythonhosted.org/mrjob/whats-new.html#ready-for-strict-protocols\n",
      "WARNING:mrjob.runner:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(33, 'A BILL FOR ESTABLISHING RELIGIOUS')\n"
     ]
    }
   ],
   "source": [
    "from hw53A import TopWords53A\n",
    "import csv\n",
    "\n",
    "mr_job = TopWords53A(args=['./loc_test.txt'])\n",
    "with mr_job.make_runner() as runner:\n",
    "    runner.run()\n",
    "    for line in runner.stream_output():\n",
    "        print mr_job.parse_output_line(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A BILL FOR ESTABLISHING RELIGIOUS\t59\t59\t54\r\n",
      "A Biography of General George\t92\t90\t74\r\n",
      "A Case Study in Government\t102\t102\t78\r\n",
      "A Case Study of Female\t447\t447\t327\r\n",
      "A Case Study of Limited\t55\t55\t43\r\n"
     ]
    }
   ],
   "source": [
    "!cat loc_test.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting hw53B.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile hw53B.py\n",
    "#!/usr/bin/python\n",
    "\"\"\"\n",
    "Top ten most frequent words. \n",
    "\"\"\"\n",
    "from mrjob.job import MRJob\n",
    "from mrjob.step import MRStep\n",
    "\n",
    "class TopWords53B(MRJob):\n",
    "    \"\"\"\n",
    "    Finds the top ten most frequent words. \n",
    "    \"\"\"\n",
    "\n",
    "    def mapper_get_words(self, _, line):\n",
    "        \"\"\"Yield words and counts.\"\"\"\n",
    "        line = line.strip()\n",
    "        line = line.lower()\n",
    "        rec = line.split(\"\\t\")\n",
    "        words = rec[0].split(\" \")\n",
    "        for word in words:\n",
    "            yield word,int(rec[1])\n",
    "                  \n",
    "\n",
    "    def reducer_count_words(self, word, counts):\n",
    "        \"\"\"Add up the counts.\"\"\"              \n",
    "        total = sum(i for i in counts)\n",
    "        yield word, total \n",
    "    \n",
    "    def reducer_find10_init(self):\n",
    "        self.count = 10000 #change back to 10\n",
    "            \n",
    "    def reducer_find_10_max(self,keyt,total):\n",
    "        \"\"\"Outputs the top ten counts and words.\"\"\"              \n",
    "        if self.count > 0:             \n",
    "            yield keyt, total.next()\n",
    "            self.count -= 1\n",
    "            \n",
    "                  \n",
    "    def steps(self):\n",
    "        return [\n",
    "            MRStep(mapper=self.mapper_get_words,\n",
    "                   reducer=self.reducer_count_words,\n",
    "                   jobconf={\n",
    "                            \"mapred.map.tasks\":28,\n",
    "                            \"mapred.reduce.tasks\":28}),\n",
    "            MRStep(#mapper=self.mapper_routekeys,\n",
    "                   reducer_init=self.reducer_find10_init,\n",
    "                   reducer=self.reducer_find_10_max,\n",
    "                   jobconf={\n",
    "                    \"stream.num.map.output.key.fields\":\"2\",\n",
    "                    \"mapreduce.job.output.key.comparator.class\":\n",
    "                        \"org.apache.hadoop.mapred.lib.KeyFieldBasedComparator\",\n",
    "                    \"mapreduce.partition.keycomparator.options\":\"-k2,2nr\",\n",
    "                    \"mapred.reduce.tasks\":1\n",
    "                          })\n",
    "        ]\n",
    "\n",
    "        \n",
    "if __name__ == '__main__':\n",
    "    TopWords53B.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!chmod a+x hw53B.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:mrjob.runner:\n",
      "WARNING:mrjob.runner:PLEASE NOTE: Starting in mrjob v0.5.0, protocols will be strict by default. It's recommended you run your job with --strict-protocols or set up mrjob.conf as described at https://pythonhosted.org/mrjob/whats-new.html#ready-for-strict-protocols\n",
      "WARNING:mrjob.runner:\n",
      "WARNING:mrjob.compat:Detected hadoop configuration property names that do not match hadoop version 0.20:\n",
      "The have been translated as follows\n",
      " mapreduce.job.output.key.comparator.class: mapred.output.key.comparator.class\n",
      "mapreduce.partition.keycomparator.options: mapred.text.key.comparator.options\n",
      "WARNING:mrjob.compat:Detected hadoop configuration property names that do not match hadoop version 0.20:\n",
      "The have been translated as follows\n",
      " mapreduce.job.output.key.comparator.class: mapred.output.key.comparator.class\n",
      "mapreduce.partition.keycomparator.options: mapred.text.key.comparator.options\n",
      "WARNING:mrjob.compat:Detected hadoop configuration property names that do not match hadoop version 0.20:\n",
      "The have been translated as follows\n",
      " mapreduce.job.output.key.comparator.class: mapred.output.key.comparator.class\n",
      "mapreduce.partition.keycomparator.options: mapred.text.key.comparator.options\n",
      "WARNING:mrjob.compat:Detected hadoop configuration property names that do not match hadoop version 0.20:\n",
      "The have been translated as follows\n",
      " mapreduce.job.output.key.comparator.class: mapred.output.key.comparator.class\n",
      "mapreduce.partition.keycomparator.options: mapred.text.key.comparator.options\n",
      "WARNING:mrjob.compat:Detected hadoop configuration property names that do not match hadoop version 0.20:\n",
      "The have been translated as follows\n",
      " mapreduce.job.output.key.comparator.class: mapred.output.key.comparator.class\n",
      "mapreduce.partition.keycomparator.options: mapred.text.key.comparator.options\n",
      "WARNING:mrjob.compat:Detected hadoop configuration property names that do not match hadoop version 0.20:\n",
      "The have been translated as follows\n",
      " mapreduce.job.output.key.comparator.class: mapred.output.key.comparator.class\n",
      "mapreduce.partition.keycomparator.options: mapred.text.key.comparator.options\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('A', 755)\n",
      "('BILL', 59)\n",
      "('Biography', 92)\n",
      "('Case', 604)\n",
      "('ESTABLISHING', 59)\n",
      "('FOR', 59)\n",
      "('Female', 447)\n",
      "('General', 92)\n",
      "('George', 92)\n",
      "('Government', 102)\n",
      "('Limited', 55)\n",
      "('RELIGIOUS', 59)\n",
      "('Study', 604)\n",
      "('in', 102)\n",
      "('of', 594)\n"
     ]
    }
   ],
   "source": [
    "from hw53B import TopWords53B\n",
    "import csv\n",
    "\n",
    "mr_job = TopWords53B(args=['./loc_test.txt'])\n",
    "with mr_job.make_runner() as runner:\n",
    "    runner.run()\n",
    "    for line in runner.stream_output():\n",
    "        print mr_job.parse_output_line(line)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Command run: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!python hw53A.py -r emr --conf-path mrjob_261mas.conf s3://filtered-5grams/ \\\n",
    "    --output-dir=s3://peridot-261-marjorie/output53a01/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%writefile hw53C.py\n",
    "#!/usr/bin/python\n",
    "\"\"\"\n",
    "Top 20 most/least densely appearing words. \n",
    "\"\"\"\n",
    "from mrjob.job import MRJob\n",
    "from mrjob.step import MRStep\n",
    "\n",
    "class TopWords53C(MRJob):\n",
    "    \"\"\"\n",
    "    TBD\n",
    "    \"\"\"\n",
    "\n",
    "    def mapper_get_words(self, _, line):\n",
    "        \"\"\"xx\"\"\"\n",
    "        line = line.strip()\n",
    "        rec = line.split(\"\\t\")\n",
    "        words = rec[0].split(\" \")\n",
    "        for word in words:\n",
    "            yield word,int(rec[1])\n",
    "                  \n",
    "\n",
    "    def reducer_count_words(self, word, counts):\n",
    "        \"\"\"xxx\"\"\"              \n",
    "        total = sum(i for i in counts)\n",
    "        yield word, total \n",
    "    \n",
    "    def reducer_find10_init(self):\n",
    "        self.count = 10\n",
    "            \n",
    "    def reducer_find_10_max(self,keyt,total):\n",
    "        \"\"\"Outputs the top ten counts and words.\"\"\"   \n",
    "        if keyt == \"*0_linelength\":\n",
    "            \n",
    "        if self.count > 0: \n",
    "            \n",
    "            yield keyt, total.next()\n",
    "            self.count -= 1\n",
    "            \n",
    "                  \n",
    "    def steps(self):\n",
    "        return [\n",
    "            MRStep(mapper=self.mapper_get_words,\n",
    "                   reducer=self.reducer_count_words),\n",
    "            MRStep(#mapper=self.mapper_routekeys,\n",
    "                   reducer_init=self.reducer_find10_init,\n",
    "                   reducer=self.reducer_find_10_max,\n",
    "                   jobconf={\n",
    "                    \"stream.num.map.output.key.fields\":\"2\",\n",
    "                    \"mapreduce.job.output.key.comparator.class\":\n",
    "                        \"org.apache.hadoop.mapred.lib.KeyFieldBasedComparator\",\n",
    "                    \"mapreduce.partition.keycomparator.options\":\"-k2,2nr\"\n",
    "                          })\n",
    "        ]\n",
    "\n",
    "        \n",
    "if __name__ == '__main__':\n",
    "    TopWords53C.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting hw53D.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile hw53D.py\n",
    "#!/usr/bin/python\n",
    "\"\"\"\n",
    "Frequency Distribution of 5-gram sizes.  \n",
    "\"\"\"\n",
    "from mrjob.job import MRJob\n",
    "from mrjob.step import MRStep\n",
    "\n",
    "class TopWords53D(MRJob):\n",
    "    \n",
    "    \n",
    "    def mapper_get_words(self, _, line):\n",
    "        \"\"\"\n",
    "        Output line lengths and lines. \n",
    "        \"\"\"\n",
    "        line = line.strip()\n",
    "        rec = line.split(\"\\t\")\n",
    "        linelen = len(rec[0])\n",
    "        yield linelen, int(rec[1]) \n",
    "                  \n",
    "\n",
    "    def reducer_count_lengths(self, linelen, count):\n",
    "        \"\"\"xxx\"\"\"              \n",
    "        total = sum(i for i in count)\n",
    "        yield linelen, total \n",
    "    \n",
    "\n",
    "                  \n",
    "    def steps(self):\n",
    "        return [\n",
    "            MRStep(mapper=self.mapper_get_words,\n",
    "                   reducer=self.reducer_count_lengths,\n",
    "                  jobconf={\n",
    "                    \"mapred.map.tasks\":28,\n",
    "                    \"mapred.reduce.tasks\":28})\n",
    "            \n",
    "        ]\n",
    "\n",
    "        \n",
    "if __name__ == '__main__':\n",
    "    TopWords53D.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!chmod a+x hw53D.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no configs found; falling back on auto-configuration\n",
      "no configs found; falling back on auto-configuration\n",
      "creating tmp directory /var/folders/5w/t79rkxsd09b8r67gk2nj_6640000gn/T/hw53D.marjoriesayer.20160215.234746.043411\n",
      "\n",
      "PLEASE NOTE: Starting in mrjob v0.5.0, protocols will be strict by default. It's recommended you run your job with --strict-protocols or set up mrjob.conf as described at https://pythonhosted.org/mrjob/whats-new.html#ready-for-strict-protocols\n",
      "\n",
      "writing to /var/folders/5w/t79rkxsd09b8r67gk2nj_6640000gn/T/hw53D.marjoriesayer.20160215.234746.043411/step-0-mapper_part-00000\n",
      "writing to /var/folders/5w/t79rkxsd09b8r67gk2nj_6640000gn/T/hw53D.marjoriesayer.20160215.234746.043411/step-0-mapper_part-00001\n",
      "writing to /var/folders/5w/t79rkxsd09b8r67gk2nj_6640000gn/T/hw53D.marjoriesayer.20160215.234746.043411/step-0-mapper_part-00002\n",
      "writing to /var/folders/5w/t79rkxsd09b8r67gk2nj_6640000gn/T/hw53D.marjoriesayer.20160215.234746.043411/step-0-mapper_part-00003\n",
      "writing to /var/folders/5w/t79rkxsd09b8r67gk2nj_6640000gn/T/hw53D.marjoriesayer.20160215.234746.043411/step-0-mapper_part-00004\n",
      "writing to /var/folders/5w/t79rkxsd09b8r67gk2nj_6640000gn/T/hw53D.marjoriesayer.20160215.234746.043411/step-0-mapper_part-00005\n",
      "writing to /var/folders/5w/t79rkxsd09b8r67gk2nj_6640000gn/T/hw53D.marjoriesayer.20160215.234746.043411/step-0-mapper_part-00006\n",
      "writing to /var/folders/5w/t79rkxsd09b8r67gk2nj_6640000gn/T/hw53D.marjoriesayer.20160215.234746.043411/step-0-mapper_part-00007\n",
      "writing to /var/folders/5w/t79rkxsd09b8r67gk2nj_6640000gn/T/hw53D.marjoriesayer.20160215.234746.043411/step-0-mapper_part-00008\n",
      "writing to /var/folders/5w/t79rkxsd09b8r67gk2nj_6640000gn/T/hw53D.marjoriesayer.20160215.234746.043411/step-0-mapper_part-00009\n",
      "writing to /var/folders/5w/t79rkxsd09b8r67gk2nj_6640000gn/T/hw53D.marjoriesayer.20160215.234746.043411/step-0-mapper_part-00010\n",
      "writing to /var/folders/5w/t79rkxsd09b8r67gk2nj_6640000gn/T/hw53D.marjoriesayer.20160215.234746.043411/step-0-mapper_part-00011\n",
      "writing to /var/folders/5w/t79rkxsd09b8r67gk2nj_6640000gn/T/hw53D.marjoriesayer.20160215.234746.043411/step-0-mapper_part-00012\n",
      "writing to /var/folders/5w/t79rkxsd09b8r67gk2nj_6640000gn/T/hw53D.marjoriesayer.20160215.234746.043411/step-0-mapper_part-00013\n",
      "writing to /var/folders/5w/t79rkxsd09b8r67gk2nj_6640000gn/T/hw53D.marjoriesayer.20160215.234746.043411/step-0-mapper_part-00014\n",
      "writing to /var/folders/5w/t79rkxsd09b8r67gk2nj_6640000gn/T/hw53D.marjoriesayer.20160215.234746.043411/step-0-mapper_part-00015\n",
      "writing to /var/folders/5w/t79rkxsd09b8r67gk2nj_6640000gn/T/hw53D.marjoriesayer.20160215.234746.043411/step-0-mapper_part-00016\n",
      "writing to /var/folders/5w/t79rkxsd09b8r67gk2nj_6640000gn/T/hw53D.marjoriesayer.20160215.234746.043411/step-0-mapper_part-00017\n",
      "writing to /var/folders/5w/t79rkxsd09b8r67gk2nj_6640000gn/T/hw53D.marjoriesayer.20160215.234746.043411/step-0-mapper_part-00018\n",
      "writing to /var/folders/5w/t79rkxsd09b8r67gk2nj_6640000gn/T/hw53D.marjoriesayer.20160215.234746.043411/step-0-mapper_part-00019\n",
      "writing to /var/folders/5w/t79rkxsd09b8r67gk2nj_6640000gn/T/hw53D.marjoriesayer.20160215.234746.043411/step-0-mapper_part-00020\n",
      "writing to /var/folders/5w/t79rkxsd09b8r67gk2nj_6640000gn/T/hw53D.marjoriesayer.20160215.234746.043411/step-0-mapper_part-00021\n",
      "writing to /var/folders/5w/t79rkxsd09b8r67gk2nj_6640000gn/T/hw53D.marjoriesayer.20160215.234746.043411/step-0-mapper_part-00022\n",
      "writing to /var/folders/5w/t79rkxsd09b8r67gk2nj_6640000gn/T/hw53D.marjoriesayer.20160215.234746.043411/step-0-mapper_part-00023\n",
      "writing to /var/folders/5w/t79rkxsd09b8r67gk2nj_6640000gn/T/hw53D.marjoriesayer.20160215.234746.043411/step-0-mapper_part-00024\n",
      "writing to /var/folders/5w/t79rkxsd09b8r67gk2nj_6640000gn/T/hw53D.marjoriesayer.20160215.234746.043411/step-0-mapper_part-00025\n",
      "writing to /var/folders/5w/t79rkxsd09b8r67gk2nj_6640000gn/T/hw53D.marjoriesayer.20160215.234746.043411/step-0-mapper_part-00026\n",
      "writing to /var/folders/5w/t79rkxsd09b8r67gk2nj_6640000gn/T/hw53D.marjoriesayer.20160215.234746.043411/step-0-mapper_part-00027\n",
      "Counters from step 1:\n",
      "  (no counters found)\n",
      "writing to /var/folders/5w/t79rkxsd09b8r67gk2nj_6640000gn/T/hw53D.marjoriesayer.20160215.234746.043411/step-0-mapper-sorted\n",
      "> sort /var/folders/5w/t79rkxsd09b8r67gk2nj_6640000gn/T/hw53D.marjoriesayer.20160215.234746.043411/step-0-mapper_part-00000 /var/folders/5w/t79rkxsd09b8r67gk2nj_6640000gn/T/hw53D.marjoriesayer.20160215.234746.043411/step-0-mapper_part-00001 /var/folders/5w/t79rkxsd09b8r67gk2nj_6640000gn/T/hw53D.marjoriesayer.20160215.234746.043411/step-0-mapper_part-00002 /var/folders/5w/t79rkxsd09b8r67gk2nj_6640000gn/T/hw53D.marjoriesayer.20160215.234746.043411/step-0-mapper_part-00003 /var/folders/5w/t79rkxsd09b8r67gk2nj_6640000gn/T/hw53D.marjoriesayer.20160215.234746.043411/step-0-mapper_part-00004 /var/folders/5w/t79rkxsd09b8r67gk2nj_6640000gn/T/hw53D.marjoriesayer.20160215.234746.043411/step-0-mapper_part-00005 /var/folders/5w/t79rkxsd09b8r67gk2nj_6640000gn/T/hw53D.marjoriesayer.20160215.234746.043411/step-0-mapper_part-00006 /var/folders/5w/t79rkxsd09b8r67gk2nj_6640000gn/T/hw53D.marjoriesayer.20160215.234746.043411/step-0-mapper_part-00007 /var/folders/5w/t79rkxsd09b8r67gk2nj_6640000gn/T/hw53D.marjoriesayer.20160215.234746.043411/step-0-mapper_part-00008 /var/folders/5w/t79rkxsd09b8r67gk2nj_6640000gn/T/hw53D.marjoriesayer.20160215.234746.043411/step-0-mapper_part-00009 /var/folders/5w/t79rkxsd09b8r67gk2nj_6640000gn/T/hw53D.marjoriesayer.20160215.234746.043411/step-0-mapper_part-00010 /var/folders/5w/t79rkxsd09b8r67gk2nj_6640000gn/T/hw53D.marjoriesayer.20160215.234746.043411/step-0-mapper_part-00011 /var/folders/5w/t79rkxsd09b8r67gk2nj_6640000gn/T/hw53D.marjoriesayer.20160215.234746.043411/step-0-mapper_part-00012 /var/folders/5w/t79rkxsd09b8r67gk2nj_6640000gn/T/hw53D.marjoriesayer.20160215.234746.043411/step-0-mapper_part-00013 /var/folders/5w/t79rkxsd09b8r67gk2nj_6640000gn/T/hw53D.marjoriesayer.20160215.234746.043411/step-0-mapper_part-00014 /var/folders/5w/t79rkxsd09b8r67gk2nj_6640000gn/T/hw53D.marjoriesayer.20160215.234746.043411/step-0-mapper_part-00015 /var/folders/5w/t79rkxsd09b8r67gk2nj_6640000gn/T/hw53D.marjoriesayer.20160215.234746.043411/step-0-mapper_part-00016 /var/folders/5w/t79rkxsd09b8r67gk2nj_6640000gn/T/hw53D.marjoriesayer.20160215.234746.043411/step-0-mapper_part-00017 /var/folders/5w/t79rkxsd09b8r67gk2nj_6640000gn/T/hw53D.marjoriesayer.20160215.234746.043411/step-0-mapper_part-00018 /var/folders/5w/t79rkxsd09b8r67gk2nj_6640000gn/T/hw53D.marjoriesayer.20160215.234746.043411/step-0-mapper_part-00019 /var/folders/5w/t79rkxsd09b8r67gk2nj_6640000gn/T/hw53D.marjoriesayer.20160215.234746.043411/step-0-mapper_part-00020 /var/folders/5w/t79rkxsd09b8r67gk2nj_6640000gn/T/hw53D.marjoriesayer.20160215.234746.043411/step-0-mapper_part-00021 /var/folders/5w/t79rkxsd09b8r67gk2nj_6640000gn/T/hw53D.marjoriesayer.20160215.234746.043411/step-0-mapper_part-00022 /var/folders/5w/t79rkxsd09b8r67gk2nj_6640000gn/T/hw53D.marjoriesayer.20160215.234746.043411/step-0-mapper_part-00023 /var/folders/5w/t79rkxsd09b8r67gk2nj_6640000gn/T/hw53D.marjoriesayer.20160215.234746.043411/step-0-mapper_part-00024 /var/folders/5w/t79rkxsd09b8r67gk2nj_6640000gn/T/hw53D.marjoriesayer.20160215.234746.043411/step-0-mapper_part-00025 /var/folders/5w/t79rkxsd09b8r67gk2nj_6640000gn/T/hw53D.marjoriesayer.20160215.234746.043411/step-0-mapper_part-00026 /var/folders/5w/t79rkxsd09b8r67gk2nj_6640000gn/T/hw53D.marjoriesayer.20160215.234746.043411/step-0-mapper_part-00027\n",
      "writing to /var/folders/5w/t79rkxsd09b8r67gk2nj_6640000gn/T/hw53D.marjoriesayer.20160215.234746.043411/step-0-reducer_part-00000\n",
      "writing to /var/folders/5w/t79rkxsd09b8r67gk2nj_6640000gn/T/hw53D.marjoriesayer.20160215.234746.043411/step-0-reducer_part-00001\n",
      "writing to /var/folders/5w/t79rkxsd09b8r67gk2nj_6640000gn/T/hw53D.marjoriesayer.20160215.234746.043411/step-0-reducer_part-00002\n",
      "writing to /var/folders/5w/t79rkxsd09b8r67gk2nj_6640000gn/T/hw53D.marjoriesayer.20160215.234746.043411/step-0-reducer_part-00003\n",
      "writing to /var/folders/5w/t79rkxsd09b8r67gk2nj_6640000gn/T/hw53D.marjoriesayer.20160215.234746.043411/step-0-reducer_part-00004\n",
      "writing to /var/folders/5w/t79rkxsd09b8r67gk2nj_6640000gn/T/hw53D.marjoriesayer.20160215.234746.043411/step-0-reducer_part-00005\n",
      "writing to /var/folders/5w/t79rkxsd09b8r67gk2nj_6640000gn/T/hw53D.marjoriesayer.20160215.234746.043411/step-0-reducer_part-00006\n",
      "writing to /var/folders/5w/t79rkxsd09b8r67gk2nj_6640000gn/T/hw53D.marjoriesayer.20160215.234746.043411/step-0-reducer_part-00007\n",
      "writing to /var/folders/5w/t79rkxsd09b8r67gk2nj_6640000gn/T/hw53D.marjoriesayer.20160215.234746.043411/step-0-reducer_part-00008\n",
      "writing to /var/folders/5w/t79rkxsd09b8r67gk2nj_6640000gn/T/hw53D.marjoriesayer.20160215.234746.043411/step-0-reducer_part-00009\n",
      "writing to /var/folders/5w/t79rkxsd09b8r67gk2nj_6640000gn/T/hw53D.marjoriesayer.20160215.234746.043411/step-0-reducer_part-00010\n",
      "writing to /var/folders/5w/t79rkxsd09b8r67gk2nj_6640000gn/T/hw53D.marjoriesayer.20160215.234746.043411/step-0-reducer_part-00011\n",
      "writing to /var/folders/5w/t79rkxsd09b8r67gk2nj_6640000gn/T/hw53D.marjoriesayer.20160215.234746.043411/step-0-reducer_part-00012\n",
      "writing to /var/folders/5w/t79rkxsd09b8r67gk2nj_6640000gn/T/hw53D.marjoriesayer.20160215.234746.043411/step-0-reducer_part-00013\n",
      "writing to /var/folders/5w/t79rkxsd09b8r67gk2nj_6640000gn/T/hw53D.marjoriesayer.20160215.234746.043411/step-0-reducer_part-00014\n",
      "writing to /var/folders/5w/t79rkxsd09b8r67gk2nj_6640000gn/T/hw53D.marjoriesayer.20160215.234746.043411/step-0-reducer_part-00015\n",
      "Counters from step 1:\n",
      "  (no counters found)\n",
      "Moving /var/folders/5w/t79rkxsd09b8r67gk2nj_6640000gn/T/hw53D.marjoriesayer.20160215.234746.043411/step-0-reducer_part-00000 -> /var/folders/5w/t79rkxsd09b8r67gk2nj_6640000gn/T/hw53D.marjoriesayer.20160215.234746.043411/output/part-00000\n",
      "Moving /var/folders/5w/t79rkxsd09b8r67gk2nj_6640000gn/T/hw53D.marjoriesayer.20160215.234746.043411/step-0-reducer_part-00001 -> /var/folders/5w/t79rkxsd09b8r67gk2nj_6640000gn/T/hw53D.marjoriesayer.20160215.234746.043411/output/part-00001\n",
      "Moving /var/folders/5w/t79rkxsd09b8r67gk2nj_6640000gn/T/hw53D.marjoriesayer.20160215.234746.043411/step-0-reducer_part-00002 -> /var/folders/5w/t79rkxsd09b8r67gk2nj_6640000gn/T/hw53D.marjoriesayer.20160215.234746.043411/output/part-00002\n",
      "Moving /var/folders/5w/t79rkxsd09b8r67gk2nj_6640000gn/T/hw53D.marjoriesayer.20160215.234746.043411/step-0-reducer_part-00003 -> /var/folders/5w/t79rkxsd09b8r67gk2nj_6640000gn/T/hw53D.marjoriesayer.20160215.234746.043411/output/part-00003\n",
      "Moving /var/folders/5w/t79rkxsd09b8r67gk2nj_6640000gn/T/hw53D.marjoriesayer.20160215.234746.043411/step-0-reducer_part-00004 -> /var/folders/5w/t79rkxsd09b8r67gk2nj_6640000gn/T/hw53D.marjoriesayer.20160215.234746.043411/output/part-00004\n",
      "Moving /var/folders/5w/t79rkxsd09b8r67gk2nj_6640000gn/T/hw53D.marjoriesayer.20160215.234746.043411/step-0-reducer_part-00005 -> /var/folders/5w/t79rkxsd09b8r67gk2nj_6640000gn/T/hw53D.marjoriesayer.20160215.234746.043411/output/part-00005\n",
      "Moving /var/folders/5w/t79rkxsd09b8r67gk2nj_6640000gn/T/hw53D.marjoriesayer.20160215.234746.043411/step-0-reducer_part-00006 -> /var/folders/5w/t79rkxsd09b8r67gk2nj_6640000gn/T/hw53D.marjoriesayer.20160215.234746.043411/output/part-00006\n",
      "Moving /var/folders/5w/t79rkxsd09b8r67gk2nj_6640000gn/T/hw53D.marjoriesayer.20160215.234746.043411/step-0-reducer_part-00007 -> /var/folders/5w/t79rkxsd09b8r67gk2nj_6640000gn/T/hw53D.marjoriesayer.20160215.234746.043411/output/part-00007\n",
      "Moving /var/folders/5w/t79rkxsd09b8r67gk2nj_6640000gn/T/hw53D.marjoriesayer.20160215.234746.043411/step-0-reducer_part-00008 -> /var/folders/5w/t79rkxsd09b8r67gk2nj_6640000gn/T/hw53D.marjoriesayer.20160215.234746.043411/output/part-00008\n",
      "Moving /var/folders/5w/t79rkxsd09b8r67gk2nj_6640000gn/T/hw53D.marjoriesayer.20160215.234746.043411/step-0-reducer_part-00009 -> /var/folders/5w/t79rkxsd09b8r67gk2nj_6640000gn/T/hw53D.marjoriesayer.20160215.234746.043411/output/part-00009\n",
      "Moving /var/folders/5w/t79rkxsd09b8r67gk2nj_6640000gn/T/hw53D.marjoriesayer.20160215.234746.043411/step-0-reducer_part-00010 -> /var/folders/5w/t79rkxsd09b8r67gk2nj_6640000gn/T/hw53D.marjoriesayer.20160215.234746.043411/output/part-00010\n",
      "Moving /var/folders/5w/t79rkxsd09b8r67gk2nj_6640000gn/T/hw53D.marjoriesayer.20160215.234746.043411/step-0-reducer_part-00011 -> /var/folders/5w/t79rkxsd09b8r67gk2nj_6640000gn/T/hw53D.marjoriesayer.20160215.234746.043411/output/part-00011\n",
      "Moving /var/folders/5w/t79rkxsd09b8r67gk2nj_6640000gn/T/hw53D.marjoriesayer.20160215.234746.043411/step-0-reducer_part-00012 -> /var/folders/5w/t79rkxsd09b8r67gk2nj_6640000gn/T/hw53D.marjoriesayer.20160215.234746.043411/output/part-00012\n",
      "Moving /var/folders/5w/t79rkxsd09b8r67gk2nj_6640000gn/T/hw53D.marjoriesayer.20160215.234746.043411/step-0-reducer_part-00013 -> /var/folders/5w/t79rkxsd09b8r67gk2nj_6640000gn/T/hw53D.marjoriesayer.20160215.234746.043411/output/part-00013\n",
      "Moving /var/folders/5w/t79rkxsd09b8r67gk2nj_6640000gn/T/hw53D.marjoriesayer.20160215.234746.043411/step-0-reducer_part-00014 -> /var/folders/5w/t79rkxsd09b8r67gk2nj_6640000gn/T/hw53D.marjoriesayer.20160215.234746.043411/output/part-00014\n",
      "Moving /var/folders/5w/t79rkxsd09b8r67gk2nj_6640000gn/T/hw53D.marjoriesayer.20160215.234746.043411/step-0-reducer_part-00015 -> /var/folders/5w/t79rkxsd09b8r67gk2nj_6640000gn/T/hw53D.marjoriesayer.20160215.234746.043411/output/part-00015\n",
      "Streaming final output from /var/folders/5w/t79rkxsd09b8r67gk2nj_6640000gn/T/hw53D.marjoriesayer.20160215.234746.043411/output\n",
      "removing tmp directory /var/folders/5w/t79rkxsd09b8r67gk2nj_6640000gn/T/hw53D.marjoriesayer.20160215.234746.043411\n"
     ]
    }
   ],
   "source": [
    "!python hw53D.py -r emr --conf-path mrjob_261mas.conf s3://filtered-5grams/ \n",
    "    --output-dir=s3://peridot-261-marjorie/output53d02\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9\t14140\r\n",
      "10\t5740\r\n",
      "11\t31532\r\n",
      "12\t114018\r\n",
      "13\t888866\r\n",
      "14\t5950911\r\n",
      "15\t23247701\r\n",
      "16\t70338372\r\n",
      "17\t171644826\r\n",
      "18\t327102659\r\n",
      "19\t519029171\r\n",
      "20\t692585590\r\n",
      "21\t825593117\r\n",
      "22\t903382557\r\n",
      "23\t921800718\r\n",
      "24\t894422079\r\n",
      "25\t826976077\r\n",
      "26\t739311284\r\n",
      "27\t636987536\r\n",
      "28\t530179866\r\n",
      "29\t433764087\r\n",
      "30\t345711670\r\n",
      "31\t269584490\r\n",
      "32\t205588578\r\n",
      "33\t155555837\r\n",
      "34\t115556130\r\n",
      "35\t84419521\r\n",
      "36\t60137979\r\n",
      "37\t44027088\r\n",
      "38\t31113761\r\n",
      "39\t22259683\r\n",
      "40\t15560400\r\n",
      "41\t10971218\r\n",
      "42\t7704330\r\n",
      "43\t5169492\r\n",
      "44\t3568517\r\n",
      "45\t2600568\r\n",
      "46\t1603291\r\n",
      "47\t1175703\r\n",
      "48\t802590\r\n",
      "49\t551091\r\n",
      "50\t373875\r\n",
      "51\t230726\r\n",
      "52\t162867\r\n",
      "53\t94986\r\n",
      "54\t64675\r\n",
      "55\t58593\r\n",
      "56\t23339\r\n",
      "57\t22210\r\n",
      "58\t14919\r\n",
      "59\t8653\r\n",
      "60\t4388\r\n",
      "61\t4984\r\n",
      "62\t2544\r\n",
      "63\t2094\r\n",
      "64\t1449\r\n",
      "65\t1263\r\n",
      "66\t460\r\n",
      "67\t365\r\n",
      "68\t211\r\n",
      "69\t230\r\n",
      "70\t375\r\n",
      "71\t292\r\n",
      "72\t334\r\n",
      "73\t182\r\n",
      "75\t87\r\n",
      "76\t157\r\n",
      "79\t83\r\n",
      "82\t95\r\n",
      "83\t51\r\n",
      "84\t421\r\n",
      "86\t91\r\n",
      "89\t92\r\n",
      "90\t84\r\n",
      "91\t155\r\n",
      "103\t91\r\n",
      "106\t90\r\n",
      "119\t148\r\n",
      "128\t92\r\n",
      "159\t182\r\n"
     ]
    }
   ],
   "source": [
    "!cat sortedD53.txt "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXMAAAEACAYAAABBDJb9AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nXTHP1EnDW+QAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x10601c810>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "xvals = []\n",
    "yvals = []\n",
    "with open(\"sortedD53.txt\", \"rb\") as f: \n",
    "    for line in f:\n",
    "        line = line.strip()\n",
    "        rec = line.split(\"\\t\")\n",
    "        xvals.append(np.log(float(rec[0])))\n",
    "        yvals.append(np.log(float(rec[1])))\n",
    "\n",
    "plt.bar(xvals, yvals)\n",
    "plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from hw53D import TopWords53D\n",
    "import csv\n",
    "\n",
    "mr_job = TopWords53D(args=['./loc_test.txt'])\n",
    "with mr_job.make_runner() as runner:\n",
    "    runner.run()\n",
    "    for line in runner.stream_output():\n",
    "        print mr_job.parse_output_line(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###HW5.4\n",
    "HW 5.4  Synonym detection over 2Gig of Data\n",
    "\n",
    "For the remainder of this assignment you will work with two datasets:\n",
    "\n",
    "### 1: unit/systems test data set: SYSTEMS TEST DATASET\n",
    "Three terms, A,B,C and their corresponding strip-docs of co-occurring terms\n",
    "\n",
    "DocA {X:20, Y:30, Z:5}\n",
    "DocB {X:100, Y:20}\n",
    "DocC {M:5, N:20, Z:5}\n",
    "\n",
    "\n",
    "### 2: A large subset of the Google n-grams dataset as was described above\n",
    "\n",
    "For each HW 5.4 -5.5 Please unit test and system test your code with respect \n",
    "to SYSTEMS TEST DATASET and show the results. \n",
    "Please compute the expected answer by hand and show your hand calculations for the \n",
    "SYSTEMS TEST DATASET. Then show the results you get with you system.\n",
    "\n",
    "In this part of the assignment we will focus on developing methods\n",
    "for detecting synonyms, using the Google 5-grams dataset. To accomplish\n",
    "this you must script two main tasks using MRJob:\n",
    "\n",
    "(1) Build stripes for the most frequent 10,000 words using cooccurence informationa based on\n",
    "the words ranked from 1001,-10,000 as a basis/vocabulary (drop stopword-like terms),\n",
    "and output to a file in your bucket on s3 (bigram analysis, though the words are non-contiguous).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hand calculation for comparison using test set:\n",
    "\n",
    "`DocA {X:20, Y:30, Z:5}`\n",
    "\n",
    "`DocB {X:100, Y:20}`\n",
    "\n",
    "`DocC {M:5, N:20, Z:5}`\n",
    "\n",
    "Using  the Jaccard similarity, we only take note of co-occurrence, not frequency. Hence the co-occurrence matrix is: \n",
    "\n",
    "`...M    N    X    Y    Z`\n",
    "\n",
    "`A  0    0    1    1    1`\n",
    "\n",
    "`B  0    0    1    1    0`\n",
    "\n",
    "`C  1    1    0    0    1`\n",
    "\n",
    "`J(A,B) = (1+1)/(1+1+1) = 2/3`\n",
    "\n",
    "`J(A,C) = 1/5`\n",
    "\n",
    "`J(B,C) = 0/5 = 0`\n",
    "\n",
    "Using cosine similarity, the co-occurence matrix is: \n",
    "\n",
    "`...M    N    X    Y    Z`\n",
    "\n",
    "`A  0    0    20   30   5` divide by sqrt(20<sup>2</sup> + 30<sup>2</sup> + 5<sup>2</sup>)\n",
    "\n",
    "`B  0    0   100   20   0`\n",
    "\n",
    "`C  5   20    0    0    5`\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cosine similarity DocA and DocB 0.700404195972\n",
      "Cosine similarity DocA and DocC 0.0323761954119\n",
      "Cosine similarity DocB and DocC 0.0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def cosine_sim(a, b):\n",
    "    numerator = 0.0\n",
    "    for e in a: \n",
    "        if e in b:\n",
    "            numerator += float(a[e]) * float(b[e])\n",
    "    def denom(x):\n",
    "        sq = [x[e]*x[e] for e in x]\n",
    "        sumsq = float(sum(sq))\n",
    "        denominator = np.sqrt(sumsq)\n",
    "        return denominator\n",
    "    denom_a = denom(a)\n",
    "    denom_b = denom(b)\n",
    "    if denom_a > 0 and denom_b > 0:\n",
    "        return numerator/denom_a/denom_b\n",
    "    \n",
    "DocA = {'X':20, 'Y':30, 'Z':5}\n",
    "\n",
    "DocB = {'X':100, 'Y':20}\n",
    "\n",
    "DocC={'M':5, 'N':20, 'Z':5}\n",
    "\n",
    "print \"Cosine similarity DocA and DocB\", cosine_sim(DocA, DocB)\n",
    "print \"Cosine similarity DocA and DocC\", cosine_sim(DocA, DocC)\n",
    "print \"Cosine similarity DocB and DocC\", cosine_sim(DocB, DocC)\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we find the top 10,000 most frequent words and their counts using the code from HW5.3B. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!python hw53B.py -r emr --conf-path mrjob_261mas.conf s3://filtered-5grams/ \n",
    "    --output-dir=s3://peridot-261-marjorie/output54a01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%writefile hw54A.py\n",
    "#!/usr/bin/python\n",
    "\"\"\"\n",
    "Build stripes for frequent words. \n",
    "\"\"\"\n",
    "from mrjob.job import MRJob\n",
    "from mrjob.step import MRStep\n",
    "\n",
    "class BuildStripes54A(MRJob):\n",
    "    \"\"\"\n",
    "    TBD\n",
    "    \"\"\"\n",
    "\n",
    "    def mapper_init(self):\n",
    "        \"\"\"\n",
    "        This makes the set of words to be used available\n",
    "        to all mappers. \n",
    "        \"\"\"\n",
    "        self.freqw = set()\n",
    "        with open(\"top_tenK.txt\", \"r\") as f:\n",
    "            for line in f:\n",
    "                line = line.strip()\n",
    "                rec = line.split(\"\\t\")\n",
    "                word = rec[0].strip('\"')\n",
    "                self.freqw.update(word)\n",
    "        \n",
    "    \n",
    "    def mapper_stripes(self, _, line):\n",
    "        \"\"\"start stripes\"\"\"\n",
    "        line = line.strip()\n",
    "        rec = line.split(\"\\t\")\n",
    "        words = rec[0].split(\" \")\n",
    "        for term in words:\n",
    "            if term not in self.freqw:\n",
    "                continue\n",
    "            for word in words:\n",
    "                if word in self.freqw and word != term:\n",
    "                    \n",
    "                \n",
    "            yield word,int(rec[1])\n",
    "                  \n",
    "\n",
    "    def reducer_count_words(self, word, counts):\n",
    "        \"\"\"xxx\"\"\"              \n",
    "        total = sum(i for i in counts)\n",
    "        yield word, total \n",
    "    \n",
    "    def reducer_find10_init(self):\n",
    "        self.count = 10\n",
    "            \n",
    "    def reducer_find_10_max(self,keyt,total):\n",
    "        \"\"\"Outputs the top ten counts and words.\"\"\"   \n",
    "        if keyt == \"*0_linelength\":\n",
    "            \n",
    "        if self.count > 0: \n",
    "            \n",
    "            yield keyt, total.next()\n",
    "            self.count -= 1\n",
    "            \n",
    "                  \n",
    "    def steps(self):\n",
    "        return [\n",
    "            MRStep(mapper=self.mapper_get_words,\n",
    "                   reducer=self.reducer_count_words),\n",
    "            MRStep(#mapper=self.mapper_routekeys,\n",
    "                   reducer_init=self.reducer_find10_init,\n",
    "                   reducer=self.reducer_find_10_max,\n",
    "                   jobconf={\n",
    "                    \"stream.num.map.output.key.fields\":\"2\",\n",
    "                    \"mapreduce.job.output.key.comparator.class\":\n",
    "                        \"org.apache.hadoop.mapred.lib.KeyFieldBasedComparator\",\n",
    "                    \"mapreduce.partition.keycomparator.options\":\"-k2,2nr\"\n",
    "                          })\n",
    "        ]\n",
    "\n",
    "        \n",
    "if __name__ == '__main__':\n",
    "    BuildStripes54A.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
