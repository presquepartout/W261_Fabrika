{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Machine Learning At Scale: HWK 4\n",
    "\n",
    "Name: Marjorie Sayer\n",
    "\n",
    "Email: 3marjorie14@gmail.com\n",
    "\n",
    "Class Name: MIDS 261 Machine Learning At Scale \n",
    "Section: 4 (Tuesday 4:00 - 5:30 PM PST)\n",
    "\n",
    "Week: 4\n",
    "\n",
    "Date: 2/11/16"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###HW 4.0. \n",
    "\n",
    "**What is MrJob? How is it different to Hadoop MapReduce?**\n",
    "MRJob is a python framework for writing MapReduce jobs. It enables the programmer to: \n",
    "\n",
    "- use Hadoop MapReduce either in the native Java framework or via Hadoop Streaming\n",
    "- easily chain a sequence of Hadoop MapReduce jobs together \n",
    "- run jobs locally or on AWS using AWS Elastic Map Reduce (EMR)\n",
    "\n",
    "**What are the mapper_init, mapper_final(), combiner_final(), reducer_final() methods? When are they called?**\n",
    "\n",
    "In the MapReduce context, the basic programming units are mappers and reducers. Combiners are optimizers for mappers. The MRJob context is larger: a MapReduce job or sequence of jobs. In this larger context, MRJob supports the setup and teardown of mapper, combiner and reducer functions. In detail: \n",
    "\n",
    "- **mapper_init()** is a method that initializes a mapper program in MRJob. If, for example, mappers require access to a stored file, the location of the file can be specified in mapper_init(). \n",
    "\n",
    "- **mapper_final()** is a mapper that is applied after mapper_init() and mapper() methods have all completed. It can be used to stream output that has been collected through mapper actions. \n",
    "\n",
    "- **combiner_final()** like mapper_final(), combiner_final() is used after a combiner() has received all of its input. The output of the combiner_final() must be the same type of key-value pairs as the output of the mapper. \n",
    "\n",
    "- **reducer_final()** is a reducer method that is applied after all input has gone to the reducer. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###HW 4.1.\n",
    "\n",
    "**What is serialization in the context of MrJob or Hadoop?**\n",
    "\n",
    "Serialization is the conversion of output data to bytes for transmission over a network. De-serialization is the conversion of bytes to input data. Both MrJob and Hadoop send data from mappers, optionally to combiners, and then to reducers. In cluster environments, mappers and reducers can be on different machines. In between stages the data is converted to bytes and reconverted for input. \n",
    "\n",
    "**When is it (serialization) used in these frameworks?**\n",
    "Serialization is necessary from mapper output to combiner or reducer input; from combiner to reducer; and from reducer to final output. \n",
    "\n",
    "**What is the default serialization mode for input and outputs for MrJob?**\n",
    "\n",
    "The default serialization mode for input and outputs for MrJob is JSON. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###HW 4.2.\n",
    "\n",
    "Recall the Microsoft logfiles data from the async lecture. The logfiles are described are located at:\n",
    "\n",
    "https://kdd.ics.uci.edu/databases/msweb/msweb.html\n",
    "http://archive.ics.uci.edu/ml/machine-learning-databases/anonymous/\n",
    "\n",
    "This dataset records which areas (Vroots) of www.microsoft.com each user visited in a one-week timeframe in Feburary 1998.\n",
    "\n",
    " Here, you must preprocess the data on a single node (i.e., not on a cluster of nodes) from the format:\n",
    "\n",
    "`C,\"10001\",10001   #Visitor id 10001`\n",
    "\n",
    "`V,1000,1          #Visit by Visitor 10001 to page id 1000`\n",
    "\n",
    "`V,1001,1          #Visit by Visitor 10001 to page id 1001`\n",
    "\n",
    "`V,1002,1          #Visit by Visitor 10001 to page id 1002`\n",
    "\n",
    "`C,\"10002\",10002   #Visitor id 10001`\n",
    "\n",
    "`V`\n",
    "\n",
    "(Note: #denotes comments) \n",
    "to the format:\n",
    "\n",
    "`V,1000,1,C, 10001`\n",
    "\n",
    "`V,1001,1,C, 10001`\n",
    "\n",
    "`V,1002,1,C, 10001`\n",
    "\n",
    "Write the python code to accomplish this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting format_data.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile format_data.py\n",
    "#!/usr/bin/python\n",
    "\"\"\"This program reformats the Microsoft anonymous visitor data.\n",
    "In particular, customer IDs and corresponding visited site IDs\n",
    "are placed on the same line. \n",
    "\"\"\"\n",
    "\n",
    "import sys\n",
    "\n",
    "checkV = False\n",
    "cstring = \"\"\n",
    "\n",
    "for line in sys.stdin:\n",
    "    \n",
    "    if line[:2] == \"V,\":\n",
    "        line = line.strip()\n",
    "        recv = line.split(\",\")\n",
    "        outstring = \"V,\" + recv[1] + \",\" + recv[2] + cstring\n",
    "        print outstring\n",
    "    elif line[:2] == \"C,\":\n",
    "        line = line.strip()\n",
    "        recc = line.split(\",\")\n",
    "        cstring = \",C, \" + recc[2] #set new customer ID string\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!chmod a+x format_data.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!cat anonymous-msweb.data | python format_data.py > hw42output.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "V,1000,1,C, 10001\r\n",
      "V,1001,1,C, 10001\r\n",
      "V,1002,1,C, 10001\r\n",
      "V,1001,1,C, 10002\r\n",
      "V,1003,1,C, 10002\r\n",
      "V,1001,1,C, 10003\r\n",
      "V,1003,1,C, 10003\r\n",
      "V,1004,1,C, 10003\r\n",
      "V,1005,1,C, 10004\r\n",
      "V,1006,1,C, 10005\r\n"
     ]
    }
   ],
   "source": [
    "!head -n10 hw42output.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###HW 4.3.\n",
    "\n",
    "Find the 5 most frequently visited pages using MrJob from the output of 4.2 (i.e., transfromed log file)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting hw43five_top.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile hw43five_top.py\n",
    "#!/usr/bin/python\n",
    "\"\"\"Find top five most frequently visited Vroots. \n",
    "This program will take the output of HW4.2 and output tab-seperated lines of\n",
    "    Vroot -> number of visits\n",
    "To run:\n",
    "    python hw43five_top.py hw42output.txt\n",
    "To store output:\n",
    "    python hw43five_top.py hw42output.txt > hw43five_top.out\n",
    "\"\"\"\n",
    "from mrjob.job import MRJob\n",
    "from mrjob.step import MRStep\n",
    "import csv\n",
    "\n",
    "def csv_readline(line):\n",
    "    \"\"\"Given a sting CSV line, return a list of strings.\"\"\"\n",
    "    for row in csv.reader([line]):\n",
    "        return row\n",
    "\n",
    "class TopPages(MRJob):\n",
    "    \"\"\"\n",
    "    SORT_VALUES = True\n",
    "\n",
    "    JOBCONF = {'mapred.output.key.comparator.class':\\\n",
    "                          'org.apache.hadoop.mapred.lib.KeyFieldBasedComparator',\\\n",
    "                          'mapred.text.key.comparator.options': '-k1nr -k2'}\n",
    "    \"\"\"\n",
    "    \n",
    "\n",
    "    def mapper_get_words(self, line_no, line):\n",
    "        \"\"\"Extracts the Vroot that was visited\"\"\"\n",
    "        cell = csv_readline(line)\n",
    "        if cell[0] == 'V':\n",
    "            yield cell[1],1\n",
    "                  # What  Key, Value  do we want to output?\n",
    "\n",
    "    def reducer_count_words(self, vroot, visit_counts):\n",
    "        \"\"\"Sumarizes the visit counts by adding them together.  If total visits\n",
    "        is more than 400, yield the results\"\"\"\n",
    "        total = sum(i for i in visit_counts)\n",
    "                # How do we calculate the total visits from the visit_counts?\n",
    "        if total >= 400:\n",
    "            yield vroot,total #know from Quiz there are > 5 with total >=400\n",
    "            \n",
    "    def mapper_routekeys(self, vroot, total):\n",
    "        \"\"\"Sends key value pairs from Job 1 to hadoop shuffle for Job 2.\"\"\"\n",
    "        yield None, (total, vroot)\n",
    "            \n",
    "    def reducer_find_5_max(self, _ , vlist):\n",
    "        \"\"\"Outputs the five Vroots with maximum visits.\"\"\"\n",
    "        output = sorted(list(vlist), reverse=True)[:5]\n",
    "        print \"Number of Visits\\tVroot ID\"\n",
    "        for item in output:\n",
    "            yield item[0], item[1]\n",
    "            \n",
    "                  \n",
    "    def steps(self):\n",
    "        return [\n",
    "            MRStep(mapper=self.mapper_get_words,\n",
    "                   reducer=self.reducer_count_words),\n",
    "            MRStep(mapper=self.mapper_routekeys,\\\n",
    "                   reducer=self.reducer_find_5_max)\n",
    "        ]\n",
    "\n",
    "        \n",
    "if __name__ == '__main__':\n",
    "    TopPages.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:mrjob.runner:\n",
      "WARNING:mrjob.runner:PLEASE NOTE: Starting in mrjob v0.5.0, protocols will be strict by default. It's recommended you run your job with --strict-protocols or set up mrjob.conf as described at https://pythonhosted.org/mrjob/whats-new.html#ready-for-strict-protocols\n",
      "WARNING:mrjob.runner:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Visits\tVroot ID\n",
      "(10836, '1008')\n",
      "(9383, '1034')\n",
      "(8463, '1004')\n",
      "(5330, '1018')\n",
      "(5108, '1017')\n"
     ]
    }
   ],
   "source": [
    "from hw43five_top import TopPages\n",
    "import csv\n",
    "\n",
    "mr_job = TopPages(args=['./hw42output.txt'])\n",
    "with mr_job.make_runner() as runner:\n",
    "    runner.run()\n",
    "    for line in runner.stream_output():\n",
    "        print mr_job.parse_output_line(line)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###HW 4.4. \n",
    "\n",
    "Find the most frequent visitor of each page using MrJob and the output of 4.2  (i.e., transfromed log file). In this output please include the webpage URL, webpageID and Visitor ID."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###HW 4.5. Clustering Tweet Dataset\n",
    "\n",
    "Using this data, you will implement a 1000-dimensional K-means algorithm in MrJob on the users\n",
    "by their 1000-dimensional word stripes/vectors using several \n",
    "centroid initializations and values of K.\n",
    "\n",
    "Note that each \"point\" is a user as represented by 1000 words, and that\n",
    "word-frequency distributions are generally heavy-tailed power-laws\n",
    "(often called Zipf distributions), and are very rare in the larger class\n",
    "of discrete, random distributions. For each user you will have to normalize\n",
    "by its \"TOTAL\" column. Try several parameterizations and initializations:\n",
    "\n",
    "(A) K=4 uniform random centroid-distributions over the 1000 words (generate 1000 random numbers and normalize the vectors)\n",
    "(B) K=2 perturbation-centroids, randomly perturbed from the aggregated (user-wide) distribution \n",
    "(C) K=4 perturbation-centroids, randomly perturbed from the aggregated (user-wide) distribution \n",
    "(D) K=4 \"trained\" centroids, determined by the sums across the classes. Use use the \n",
    "(row-normalized) class-level aggregates as 'trained' starting centroids (i.e., the training is already done for you!)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%writefile Kmeans.py\n",
    "from numpy import argmin, array, random\n",
    "from mrjob.job import MRJob\n",
    "from mrjob.step import MRJobStep\n",
    "from itertools import chain\n",
    "\n",
    "#Calculate the nearest centroid for data point \n",
    "def MinDist(datapoint, centroid_points):\n",
    "    datapoint = array(datapoint)\n",
    "    centroid_points = array(centroid_points)\n",
    "    diff = datapoint - centroid_points \n",
    "    diffsq = diff*diff\n",
    "    # Get the nearest centroid for each instance\n",
    "    minidx = argmin(list(diffsq.sum(axis = 1)))\n",
    "    return minidx\n",
    "\n",
    "#Check whether centroids converge\n",
    "def stop_criterion(centroid_points_old, centroid_points_new,T):\n",
    "    oldvalue = list(chain(*centroid_points_old))\n",
    "    newvalue = list(chain(*centroid_points_new))\n",
    "    Diff = [abs(x-y) for x, y in zip(oldvalue, newvalue)]\n",
    "    Flag = True\n",
    "    for i in Diff:\n",
    "        if(i>T):\n",
    "            Flag = False\n",
    "            break\n",
    "    return Flag\n",
    "\n",
    "class MRKmeans(MRJob):\n",
    "    centroid_points=[]\n",
    "    k=3    \n",
    "    def steps(self):\n",
    "        return [\n",
    "            MRJobStep(mapper_init = self.mapper_init, mapper=self.mapper,combiner = self.combiner,reducer=self.reducer)\n",
    "               ]\n",
    "    #load centroids info from file\n",
    "    def mapper_init(self):\n",
    "        self.centroid_points = [map(float,s.split('\\n')[0].split(',')) for s in open(\"Centroids.txt\").readlines()]\n",
    "        open('Centroids.txt', 'w').close()\n",
    "    #load data and output the nearest centroid index and data point \n",
    "    def mapper(self, _, line):\n",
    "        D = (map(float,line.split(',')))\n",
    "        yield int(MinDist(D,self.centroid_points)), (D[0],D[1],1)\n",
    "    #Combine sum of data points locally\n",
    "    def combiner(self, idx, inputdata):\n",
    "        sumx = sumy = num = 0\n",
    "        for x,y,n in inputdata:\n",
    "            num = num + n\n",
    "            sumx = sumx + x\n",
    "            sumy = sumy + y\n",
    "        yield idx,(sumx,sumy,num)\n",
    "    #Aggregate sum for each cluster and then calculate the new centroids\n",
    "    def reducer(self, idx, inputdata): \n",
    "        centroids = []\n",
    "        num = [0]*self.k \n",
    "        for i in range(self.k):\n",
    "            centroids.append([0,0])\n",
    "        for x, y, n in inputdata:\n",
    "            num[idx] = num[idx] + n\n",
    "            centroids[idx][0] = centroids[idx][0] + x\n",
    "            centroids[idx][1] = centroids[idx][1] + y\n",
    "        centroids[idx][0] = centroids[idx][0]/num[idx]\n",
    "        centroids[idx][1] = centroids[idx][1]/num[idx]\n",
    "        with open('Centroids.txt', 'a') as f:\n",
    "            f.writelines(str(centroids[idx][0]) + ',' + str(centroids[idx][1]) + '\\n')\n",
    "        yield idx,(centroids[idx][0],centroids[idx][1])\n",
    "      \n",
    "if __name__ == '__main__':\n",
    "    MRKmeans.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Driver\n",
    "\n",
    "Generate random initial centroids\n",
    "\n",
    "New Centroids = initial centroids\n",
    "\n",
    "While(1)ï¼š\n",
    "\n",
    "Cacluate new centroids\n",
    "\n",
    "stop if new centroids close to old centroids\n",
    "\n",
    "Updates centroids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from numpy import random, array\n",
    "from Kmeans import MRKmeans, stop_criterion\n",
    "mr_job = MRKmeans(args=['Kmeandata.csv'])\n",
    "\n",
    "#Generate k initial centroids\n",
    "def init_rand_centroids(k,n):\n",
    "    \"\"\"\n",
    "    Initialize k centroids of dimension n. \n",
    "    For HW4.5, n is 1000. \n",
    "    \"\"\"\n",
    "    centroid_points = []    \n",
    "    for i in range(k):\n",
    "        inputv = [random.uniform() for j in range(n)]\n",
    "        total = sum(inputv)\n",
    "        inputv = array(inputv)\n",
    "        inputv = inputv / total\n",
    "        centroid_points.append(inputv)\n",
    "    return centroid_points\n",
    "\n",
    "def startCentroidsBC(k, n):\n",
    "    counter = 0\n",
    "    for line in open(\"topUsers_Apr-Jul_2014_1000-words_summaries.txt\").readlines():\n",
    "        if counter == 2:        \n",
    "            data = re.split(\",\",line)\n",
    "            globalAggregate = [float(data[i+3])/float(data[2]) for i in range(n)]\n",
    "        counter += 1\n",
    "    ## perturb the global aggregate for the four initializations    \n",
    "    centroids = []\n",
    "    for i in range(k):\n",
    "        rndpoints = random.sample(n)\n",
    "        peturpoints = [rndpoints[n]/10+globalAggregate[n] for n in range(n)]\n",
    "        centroids.append(peturpoints)\n",
    "        total = 0\n",
    "        for j in range(len(centroids[i])):\n",
    "            total += centroids[i][j]\n",
    "        for j in range(len(centroids[i])):\n",
    "            centroids[i][j] = centroids[i][j]/total\n",
    "    return centroids\n",
    "\n",
    "def trained_centroids(k, n):\n",
    "    pass\n",
    "\n",
    "def initialize_centroids(k, n, flavor):\n",
    "    \"\"\"\n",
    "    Initializes k centroids of dimension n, using the algorithm specified in \"flavor\". \n",
    "    \"\"\"\n",
    "    if flavor == \"random\":\n",
    "        centroid_points = init_rand_centroids(k,n)\n",
    "    elif flavor == \"perturb\":\n",
    "        centroid_points = startCentroidsBC(k, n)\n",
    "    elif flavor == \"trained\":\n",
    "        centroid_points = trained_centroids(k, n)\n",
    "        \n",
    "    with open('Centroids.txt', 'w+') as f:\n",
    "        f.writelines(','.join(str(j) for j in i) + '\\n' for i in centroid_points)\n",
    "    f.close()\n",
    "    \n",
    "# Initialize centroids. \n",
    "k = 4 #can be 4 or 2\n",
    "n = 1000\n",
    "flavor = \"random\" #can be \"random\" \"perturb\" \"trained\"\n",
    "intitalize_centroids(k, n, flavor)\n",
    "\n",
    "# Update centroids iteratively\n",
    "i = 0\n",
    "while(1):\n",
    "    # save previous centoids to check convergency\n",
    "    centroid_points_old = centroid_points[:]\n",
    "    print \"iteration\"+str(i)+\":\"\n",
    "    with mr_job.make_runner() as runner: \n",
    "        runner.run()\n",
    "        # stream_output: get access of the output \n",
    "        for line in runner.stream_output():\n",
    "            key,value =  mr_job.parse_output_line(line)\n",
    "            print key, value\n",
    "            centroid_points[key] = value\n",
    "    print \"\\n\"\n",
    "    i = i + 1\n",
    "    if(stop_criterion(centroid_points_old,centroid_points,0.01)):\n",
    "        break\n",
    "print \"Centroids\\n\"\n",
    "print centroid_points #within reducer, intermediate centroids written to file"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
