{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##DATSCIW261 ASSIGNMENT #5\n",
    "\n",
    "MIDS UC Berkeley, Machine Learning at Scale\n",
    "\n",
    "DATSCIW261 ASSIGNMENT #5\n",
    "\n",
    "TEAM 4: Hetal Chandaria (hetalchandaria@berkeley.edu), Marjorie Sayer (3marjorie14@gmail.com), Patrick Ng (patng323@gmail.com)\n",
    "\n",
    "Version 2016-02-19 (FINAL)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###HW 5.0\n",
    "\n",
    "**What is a data warehouse?** \n",
    "\n",
    "A data warehouse is a central repository of integrated data used for reporting and analysis purposes. Typically the data is loaded into the warehouse from disparate sources, and the data undergoes preliminary processing to make it report-ready or analysis-ready. This process is referred to as \"ETL\" - Extract, Transform, and Load. \n",
    "\n",
    "An example of a data warehouse for a large company might be: data from sales, manufacturing, R&D, service, finance are all stored in the warehouse. The transform process might normalize data, one-hot encode data, and so on. \n",
    "\n",
    "**What is a Star schema? When is it used?**\n",
    "\n",
    "A Star Schema is a star-shaped denormalized database schema that consists of two types of tables: Fact tables and Dimension tables. Fact tables contain measurable, quantitative data and Dimension tables contain descriptive data. For example, a fact table for a toy store would contain units made on particular dates, units sold on dates, where units are distributed. Corresponding dimension tables would contain toy models, colors, sizes, and other characteristics of the toys. \n",
    "\n",
    "Typically Dimension tables have fewer records but more attributes than Fact tables. Fact tables lie in the center of the Star Schema, with associated Dimension tables branching out from the central Fact tables. \n",
    "\n",
    "Star Schemas are useful in cases where certain queries about transactions are made frequently. For example, suppose a business sells Products in a collection of Stores. There would be many queries of a particular type such as how many products sold in certain periods; how many sold by location. It would make sense to build a Star Schema with transactions as the Fact table, and Dimension tables such as Date, Store, and Product. The Dimension tables would be small but hold all particulars about each dimension. \n",
    "\n",
    "In a MapReduce context, Dimension Tables could be held in memory or read into a hash table and the far larger Fact table used for streaming input. \n",
    "\n",
    "If relationships become too complex, a Star Schema might not be as useful. Star Schemas, since they are not normalized, have a risk of becoming inconsistent. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###HW 5.1\n",
    "\n",
    "**In the database world What is 3NF?** \n",
    "3NF stands for third normal form. A set of database tables is normalized if redundant information is removed. This aids in database consistency and accuracy; if values are repeated, they can get out of sync. There are several levels of normalization. \n",
    "First normal form: a table is in first normal form if and only if the domain of each attribute contains only atomic (indivisible) values, and the value of each attribute contains only a single value from that domain.\n",
    "\n",
    "Second normal form: a table is in second normal form if it is in first normal form and no non-prime attribute is dependent on any proper subset of any candidate key of the table. A non-prime attribute of a table is an attribute that is not a part of any candidate key of the table. (A candidate key uniquely defines rows in the table). \n",
    "\n",
    "A table R is in 3NF if the following two conditions hold: \n",
    "\n",
    "- The relation R (table) is in second normal form (2NF)\n",
    "- Every non-prime attribute of R is non-transitively dependent on every key of R.\n",
    "\n",
    "3NF is a more rigorous normalization condition than first or second normal form. \n",
    "\n",
    "**Does machine learning use data in 3NF? If so why?**\n",
    "Machine learning learns from samples. The samples have characteristics that a model can learn. For machine learning it is best to represent data with all of its characteristics present. In 3NF, information can be spread out in different tables. \n",
    "\n",
    "**In what form does ML consume data?**\n",
    "ML consumes data as samples with collections of features. \n",
    "\n",
    "**Why would one use log files that are denormalized?**\n",
    "To assemble all the features that could influence a model together. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HW 5.2\n",
    "Using MRJob, implement a hashside join (memory-backed map-side) for left, \n",
    "right and inner joins. Run your code on the  data used in HW 4.4: (Recall HW 4.4: Find the most frequent visitor of each page using mrjob and the output of 4.2  (i.e., transfromed log file). In this output please include the webpage URL, webpageID and Visitor ID.)\n",
    ":\n",
    "\n",
    "Justify which table you chose as the Left table in this hashside join.\n",
    "\n",
    "Please report the number of rows resulting from:\n",
    "\n",
    "(1) Left joining Table Left with Table Right\n",
    "\n",
    "(2) Right joining Table Left with Table Right\n",
    "\n",
    "(3) Inner joining Table Left with Table Right\n",
    "\n",
    "<span style=\"color: blue\"> Answer: </span>\n",
    "\n",
    "left table = url table of the format page id, url (294 entries)\n",
    "\n",
    "right table = log table of the format V,page id, C, customer id (98,654 entries)\n",
    " \n",
    "\n",
    "The left table is chosen to be the smaller table, because in memory backed map-side joins the left table is the one held in memory.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting MRJoin_5_2.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile MRJoin_5_2.py\n",
    "\n",
    "from mrjob.job import MRJob, MRStep\n",
    "import mrjob\n",
    "import csv\n",
    "import sys\n",
    "\n",
    "class MRJoin(MRJob):\n",
    "    urls = {} # key = pageId, value = url\n",
    "    keys_emitted = set() # Set of keys of all emitted urls. Used for left join.\n",
    "    \n",
    "    def configure_options(self):\n",
    "        super(MRJoin, self).configure_options()\n",
    "        self.add_passthrough_option(\n",
    "            '--joinType', type='str', default=\"inner\")\n",
    "        \n",
    "    def load_options(self, args):\n",
    "        super(MRJoin, self).load_options(args)\n",
    "        self.joinType = self.options.joinType\n",
    "        \n",
    "    def mapper_init(self):\n",
    "        # Load URL info data file into memory.  \n",
    "        # Line format: \n",
    "        # 1287,/autoroute\n",
    "        with open(\"hw52input.txt\", \"r\") as f:\n",
    "            for fields in csv.reader(f):\n",
    "                self.urls[fields[0]] = fields[1]\n",
    "\n",
    "    def mapper(self, line_no, line):\n",
    "        # Line format: \n",
    "        # V,1000,1,C,10001\n",
    "        fields = csv.reader([line]).next()\n",
    "        \n",
    "        key = fields[1]\n",
    "        url = None\n",
    "        toEmit = False\n",
    "        \n",
    "        if key in self.urls:\n",
    "            url = self.urls[key]\n",
    "            \n",
    "        if self.joinType == \"right\":\n",
    "            toEmit = True\n",
    "        elif self.joinType == \"left\":\n",
    "            if url is not None:\n",
    "                toEmit = True\n",
    "                self.keys_emitted.add(key) # Remember what we have emitted\n",
    "        else: # inner join\n",
    "            if url is not None:\n",
    "                toEmit = True\n",
    "        \n",
    "        if toEmit:\n",
    "            # Output format\n",
    "            # pageid, url,V,1,C,10001\n",
    "            yield key, (url, fields[0], fields[2], fields[3], fields[4])\n",
    "        \n",
    "    def mapper_final(self):\n",
    "        if self.joinType == \"left\":\n",
    "            # Emit all the remaining urls\n",
    "            remaining = set(self.urls.keys()) - self.keys_emitted\n",
    "            for key in remaining:\n",
    "                yield key, (self.urls[key], None, None, None, None)\n",
    "                                \n",
    "    def steps(self):\n",
    "        return [\n",
    "            MRStep(mapper_init=self.mapper_init,\n",
    "                   mapper=self.mapper,\n",
    "                   mapper_final=self.mapper_final)\n",
    "            ]\n",
    "    \n",
    "if __name__ == '__main__':\n",
    "    MRJoin.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HW5.2 Summary Results\n",
      "\n",
      "Join type:left\n",
      "Number of records:98663\n",
      "First 5 lines:\n",
      "\"1000\"\t[\"www.microsoft.com/regwiz\", \"V\", \"1\", \"C\", \" 10001\"]\n",
      "\"1001\"\t[\"www.microsoft.com/support\", \"V\", \"1\", \"C\", \" 10001\"]\n",
      "\"1002\"\t[\"www.microsoft.com/athome\", \"V\", \"1\", \"C\", \" 10001\"]\n",
      "\"1001\"\t[\"www.microsoft.com/support\", \"V\", \"1\", \"C\", \" 10002\"]\n",
      "\"1003\"\t[\"www.microsoft.com/kb\", \"V\", \"1\", \"C\", \" 10002\"]\n",
      "\n",
      "Join type:right\n",
      "Number of records:98654\n",
      "First 5 lines:\n",
      "\"1000\"\t[\"www.microsoft.com/regwiz\", \"V\", \"1\", \"C\", \" 10001\"]\n",
      "\"1001\"\t[\"www.microsoft.com/support\", \"V\", \"1\", \"C\", \" 10001\"]\n",
      "\"1002\"\t[\"www.microsoft.com/athome\", \"V\", \"1\", \"C\", \" 10001\"]\n",
      "\"1001\"\t[\"www.microsoft.com/support\", \"V\", \"1\", \"C\", \" 10002\"]\n",
      "\"1003\"\t[\"www.microsoft.com/kb\", \"V\", \"1\", \"C\", \" 10002\"]\n",
      "\n",
      "Join type:inner\n",
      "Number of records:98654\n",
      "First 5 lines:\n",
      "\"1000\"\t[\"www.microsoft.com/regwiz\", \"V\", \"1\", \"C\", \" 10001\"]\n",
      "\"1001\"\t[\"www.microsoft.com/support\", \"V\", \"1\", \"C\", \" 10001\"]\n",
      "\"1002\"\t[\"www.microsoft.com/athome\", \"V\", \"1\", \"C\", \" 10001\"]\n",
      "\"1001\"\t[\"www.microsoft.com/support\", \"V\", \"1\", \"C\", \" 10002\"]\n",
      "\"1003\"\t[\"www.microsoft.com/kb\", \"V\", \"1\", \"C\", \" 10002\"]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from MRJoin_5_2 import MRJoin\n",
    "\n",
    "print \"HW5.2 Summary Results\\n\"\n",
    "\n",
    "for joinType in [\"left\", \"right\", \"inner\"]:\n",
    "    mr_job = MRJoin(args=['hw42output.txt', \n",
    "                        '--file', 'hw52input.txt', # broadcast to every mapper\n",
    "                        \"--strict-protocols\",\n",
    "                        '--joinType', joinType])\n",
    "\n",
    "    with mr_job.make_runner() as runner: \n",
    "        runner.run()\n",
    "\n",
    "        lines = []\n",
    "        for line in runner.stream_output():\n",
    "            lines.append(line)\n",
    "            \n",
    "        print \"Join type:\" + joinType\n",
    "        print \"Number of records:\" + str(len(lines))\n",
    "        print \"First 5 lines:\"\n",
    "        for i in range(5):\n",
    "            print lines[i].strip()\n",
    "            \n",
    "        print"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### HW 5.3 \n",
    "\n",
    "HW 5.3  EDA of Google n-grams dataset\n",
    "For the Google n-grams dataset unit test and regression test your code using the \n",
    "first 10 lines of the following file:\n",
    "\n",
    "googlebooks-eng-all-5gram-20090715-0-filtered.txt\n",
    "\n",
    "Finally show your results on the Google n-grams dataset. \n",
    "\n",
    "\n",
    "In particular, this bucket contains (~200) files (10Meg each) in the format:\n",
    "\n",
    "\t(ngram) \\t (count) \\t (pages_count) \\t (books_count)\n",
    "\n",
    "Do some EDA on this dataset using mrjob, e.g., \n",
    "\n",
    "- Longest 5-gram (number of characters)\n",
    "- Top 10 most frequent words (please use the count information), i.e., unigrams\n",
    "- 20 Most/Least densely appearing words (count/pages_count) sorted in decreasing order of relative frequency \n",
    "- Distribution of 5-gram sizes (character length).  E.g., count (using the count field) up how many times a 5-gram of 50 characters shows up. Plot the data graphically using a histogram.\n",
    "\n",
    "HW 5.3.1 OPTIONAL Question:\n",
    "- Plot the log-log plot of the frequency distributuion of unigrams. Does it follow power law distribution?\n",
    "\n",
    "For more background see:\n",
    "https://en.wikipedia.org/wiki/Log%E2%80%93log_plot\n",
    "\n",
    "https://en.wikipedia.org/wiki/Power_law"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Longest 5-gram (number of characters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting longestngram.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile longestngram.py\n",
    "\n",
    "from mrjob.job import MRJob\n",
    "from mrjob.step import MRStep\n",
    "import csv, re\n",
    "from mrjob.protocol import RawProtocol, ReprProtocol\n",
    "\n",
    "class Longest(MRJob):\n",
    "    def steps(self):\n",
    "        return [\n",
    "            MRStep(mapper=self.mapper, combiner = self.combiner, reducer = self.reducer)\n",
    "               ]\n",
    "    \n",
    "    def mapper(self, line_no, line):\n",
    "        ngram,occurence,page_count,book_count = line.strip().split('\\t')\n",
    "        yield None,(len(ngram),ngram)\n",
    "        \n",
    "    def combiner(self,_,value):\n",
    "        yield None, (max(value))\n",
    "        \n",
    "    def reducer(self,_,value):\n",
    "        yield max(value)\n",
    "                          \n",
    "if __name__ == '__main__':\n",
    "    Longest.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:mrjob.runner:\n",
      "WARNING:mrjob.runner:PLEASE NOTE: Starting in mrjob v0.5.0, protocols will be strict by default. It's recommended you run your job with --strict-protocols or set up mrjob.conf as described at https://pythonhosted.org/mrjob/whats-new.html#ready-for-strict-protocols\n",
      "WARNING:mrjob.runner:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n",
      "Longest 5-gram is \n",
      "(58, 'Interpersonal Communication Interpersonal communication is')\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from longestngram import Longest\n",
    "import csv\n",
    "\n",
    "mr_job = Longest(args=['filtered-5Grams/googlebooks-eng-all-5gram-20090715-0-filtered.txt'])\n",
    "with mr_job.make_runner() as runner:\n",
    "    runner.run()\n",
    "    print \"Longest 5-gram is \"\n",
    "    for line in runner.stream_output():\n",
    "        print mr_job.parse_output_line(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:mrjob.conf:Got unexpected keyword arguments: ssh_tunnel\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Longest 5-gram is \n",
      "(159, 'ROPLEZIMPREDASTRODONBRASLPKLSON YHROACLMPARCHEYXMMIOUDAVESAURUS PIOFPILOCOWERSURUASOGETSESNEGCP TYRAVOPSIFENGOQUAPIALLOBOSKENUO OWINFUYAIOKENECKSASXHYILPOYNUAT')\n"
     ]
    }
   ],
   "source": [
    "from longestngram import Longest\n",
    "\n",
    "mr_job = Longest(args=['s3://filtered-5grams/', '-r', 'emr', '--no-output'])\n",
    "\n",
    "with mr_job.make_runner() as runner: \n",
    "    runner.run()\n",
    "    print \"Longest 5-gram is \"\n",
    "    for line in runner.stream_output():\n",
    "        print mr_job.parse_output_line(line)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### Top 10 most frequent words (count), i.e., unigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting frequentwords.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile frequentwords.py\n",
    "\n",
    "from mrjob.job import MRJob\n",
    "from mrjob.step import MRStep\n",
    "import csv, re\n",
    "from mrjob.protocol import RawProtocol, ReprProtocol\n",
    "\n",
    "WORD_RE = re.compile(r\"[\\w']+\")\n",
    "\n",
    "class FrequentWords(MRJob):\n",
    "    def steps(self):\n",
    "        return [\n",
    "            MRStep(mapper=self.mapper, combiner = self.combiner, reducer = self.reducer)\n",
    "               ]\n",
    "    \n",
    "    def mapper(self, line_no, line):\n",
    "        ngram,occurence,page_count,book_count = line.strip().split('\\t')\n",
    "        words = re.findall(WORD_RE,ngram)\n",
    "        for word in words:\n",
    "            yield word.lower(),int(occurence)\n",
    "        \n",
    "    def combiner(self,word,value):\n",
    "        yield word, sum(value)\n",
    "        \n",
    "    def reducer(self, word ,value):\n",
    "        yield word, sum(value)\n",
    "                          \n",
    "if __name__ == '__main__':\n",
    "    FrequentWords.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!python ./frequentwords.py  filtered-5Grams/googlebooks-eng-all-5gram-20090715-0-filtered.txt -q >words.out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"the\"\t27691943\r\n",
      "\"of\"\t18590950\r\n",
      "\"to\"\t11601757\r\n",
      "\"in\"\t7470912\r\n",
      "\"a\"\t6926743\r\n",
      "\"and\"\t6150529\r\n",
      "\"that\"\t4077421\r\n",
      "\"is\"\t4074864\r\n",
      "\"be\"\t3720812\r\n",
      "\"was\"\t2492074\r\n"
     ]
    }
   ],
   "source": [
    "!sort -k2 -n -r words.out > top10.out\n",
    "! head -10 top10.out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Got unexpected keyword arguments: ssh_tunnel\n",
      "using configs in /Users/hetal/.mrjob.conf\n",
      "using existing scratch bucket mrjob-effea262003c6b3a\n",
      "using s3://mrjob-effea262003c6b3a/tmp/ as our scratch dir on S3\n",
      "creating tmp directory /var/folders/91/cjfxt7ys6958qll6vjtgwwfw0000gn/T/frequentwords.hetal.20160215.024543.071579\n",
      "writing master bootstrap script to /var/folders/91/cjfxt7ys6958qll6vjtgwwfw0000gn/T/frequentwords.hetal.20160215.024543.071579/b.py\n",
      "Copying non-input files into s3://mrjob-effea262003c6b3a/tmp/frequentwords.hetal.20160215.024543.071579/files/\n",
      "Waiting 5.0s for S3 eventual consistency\n",
      "Creating Elastic MapReduce job flow\n",
      "Job flow created with ID: j-K0ZK6SXTK7C0\n",
      "Created new job flow j-K0ZK6SXTK7C0\n",
      "Job launched 30.8s ago, status STARTING: Provisioning Amazon EC2 capacity\n",
      "Job launched 62.4s ago, status STARTING: Provisioning Amazon EC2 capacity\n",
      "Job launched 93.2s ago, status STARTING: Provisioning Amazon EC2 capacity\n",
      "Job launched 124.6s ago, status STARTING: Provisioning Amazon EC2 capacity\n",
      "Job launched 155.5s ago, status STARTING: Configuring cluster software\n",
      "Job launched 186.9s ago, status BOOTSTRAPPING: Running bootstrap actions\n",
      "Job launched 217.7s ago, status BOOTSTRAPPING: Running bootstrap actions\n",
      "Job launched 249.3s ago, status RUNNING: Running step (frequentwords.hetal.20160215.024543.071579: Step 1 of 1)\n",
      "Job launched 280.1s ago, status RUNNING: Running step (frequentwords.hetal.20160215.024543.071579: Step 1 of 1)\n",
      "Job launched 311.5s ago, status RUNNING: Running step (frequentwords.hetal.20160215.024543.071579: Step 1 of 1)\n",
      "Job launched 342.4s ago, status RUNNING: Running step (frequentwords.hetal.20160215.024543.071579: Step 1 of 1)\n",
      "Job launched 373.9s ago, status RUNNING: Running step (frequentwords.hetal.20160215.024543.071579: Step 1 of 1)\n",
      "Job launched 404.7s ago, status RUNNING: Running step (frequentwords.hetal.20160215.024543.071579: Step 1 of 1)\n",
      "Job launched 436.0s ago, status RUNNING: Running step (frequentwords.hetal.20160215.024543.071579: Step 1 of 1)\n",
      "Job launched 466.9s ago, status RUNNING: Running step (frequentwords.hetal.20160215.024543.071579: Step 1 of 1)\n",
      "Job launched 498.4s ago, status RUNNING: Running step (frequentwords.hetal.20160215.024543.071579: Step 1 of 1)\n",
      "Job launched 529.2s ago, status RUNNING: Running step (frequentwords.hetal.20160215.024543.071579: Step 1 of 1)\n",
      "Job launched 560.9s ago, status RUNNING: Running step (frequentwords.hetal.20160215.024543.071579: Step 1 of 1)\n",
      "Job launched 591.7s ago, status RUNNING: Running step (frequentwords.hetal.20160215.024543.071579: Step 1 of 1)\n",
      "Job launched 623.0s ago, status RUNNING: Running step (frequentwords.hetal.20160215.024543.071579: Step 1 of 1)\n",
      "Job launched 653.9s ago, status RUNNING: Running step (frequentwords.hetal.20160215.024543.071579: Step 1 of 1)\n",
      "Job launched 685.3s ago, status RUNNING: Running step (frequentwords.hetal.20160215.024543.071579: Step 1 of 1)\n",
      "Job launched 716.2s ago, status RUNNING: Running step (frequentwords.hetal.20160215.024543.071579: Step 1 of 1)\n",
      "Job launched 747.6s ago, status RUNNING: Running step (frequentwords.hetal.20160215.024543.071579: Step 1 of 1)\n",
      "Job launched 778.5s ago, status RUNNING: Running step (frequentwords.hetal.20160215.024543.071579: Step 1 of 1)\n",
      "Job launched 810.0s ago, status RUNNING: Running step (frequentwords.hetal.20160215.024543.071579: Step 1 of 1)\n",
      "Job launched 840.9s ago, status RUNNING: Running step (frequentwords.hetal.20160215.024543.071579: Step 1 of 1)\n",
      "Job launched 872.3s ago, status RUNNING: Running step (frequentwords.hetal.20160215.024543.071579: Step 1 of 1)\n",
      "Job launched 903.1s ago, status RUNNING: Running step (frequentwords.hetal.20160215.024543.071579: Step 1 of 1)\n",
      "Job launched 934.5s ago, status RUNNING: Running step (frequentwords.hetal.20160215.024543.071579: Step 1 of 1)\n",
      "Job launched 965.4s ago, status RUNNING: Running step (frequentwords.hetal.20160215.024543.071579: Step 1 of 1)\n",
      "Job launched 996.7s ago, status RUNNING: Running step (frequentwords.hetal.20160215.024543.071579: Step 1 of 1)\n",
      "Job launched 1028.0s ago, status RUNNING: Running step (frequentwords.hetal.20160215.024543.071579: Step 1 of 1)\n",
      "Job launched 1059.3s ago, status RUNNING: Running step (frequentwords.hetal.20160215.024543.071579: Step 1 of 1)\n",
      "Job launched 1090.7s ago, status RUNNING: Running step (frequentwords.hetal.20160215.024543.071579: Step 1 of 1)\n",
      "Job launched 1122.0s ago, status RUNNING: Running step (frequentwords.hetal.20160215.024543.071579: Step 1 of 1)\n",
      "Job completed.\n",
      "Running time was 899.0s (not counting time spent waiting for the EC2 instances)\n",
      "ec2_key_pair_file not specified, going to S3\n",
      "Fetching counters from S3...\n",
      "Waiting 5.0s for S3 eventual consistency\n",
      "Counters from step 1:\n",
      "  File Input Format Counters :\n",
      "    Bytes Read: 2156069116\n",
      "  File Output Format Counters :\n",
      "    Bytes Written: 4158739\n",
      "  FileSystemCounters:\n",
      "    FILE_BYTES_READ: 514833288\n",
      "    FILE_BYTES_WRITTEN: 293896827\n",
      "    HDFS_BYTES_READ: 23640\n",
      "    S3_BYTES_READ: 2156069116\n",
      "    S3_BYTES_WRITTEN: 4158739\n",
      "  Job Counters :\n",
      "    Launched map tasks: 193\n",
      "    Launched reduce tasks: 17\n",
      "    Rack-local map tasks: 191\n",
      "    SLOTS_MILLIS_MAPS: 19020299\n",
      "    SLOTS_MILLIS_REDUCES: 5892201\n",
      "    Total time spent by all maps waiting after reserving slots (ms): 0\n",
      "    Total time spent by all reduces waiting after reserving slots (ms): 0\n",
      "  Map-Reduce Framework:\n",
      "    CPU time spent (ms): 7533240\n",
      "    Combine input records: 306120824\n",
      "    Combine output records: 19532239\n",
      "    Map input bytes: 2156069116\n",
      "    Map input records: 58682266\n",
      "    Map output bytes: 3136729760\n",
      "    Map output materialized bytes: 87637229\n",
      "    Map output records: 293411330\n",
      "    Physical memory (bytes) snapshot: 142452686848\n",
      "    Reduce input groups: 269339\n",
      "    Reduce input records: 6822745\n",
      "    Reduce output records: 269339\n",
      "    Reduce shuffle bytes: 87637229\n",
      "    SPLIT_RAW_BYTES: 23640\n",
      "    Spilled Records: 26354984\n",
      "    Total committed heap usage (bytes): 143728836608\n",
      "    Virtual memory (bytes) snapshot: 397849792512\n",
      "removing tmp directory /var/folders/91/cjfxt7ys6958qll6vjtgwwfw0000gn/T/frequentwords.hetal.20160215.024543.071579\n",
      "Removing all files in s3://mrjob-effea262003c6b3a/tmp/frequentwords.hetal.20160215.024543.071579/\n",
      "Removing all files in s3://hvcemrbucket/logs/j-K0ZK6SXTK7C0/\n",
      "Terminating job flow: j-K0ZK6SXTK7C0\n"
     ]
    }
   ],
   "source": [
    "!python ./frequentwords.py \\\n",
    "    -r emr s3://filtered-5grams \\\n",
    "    --output-dir=s3://hvcemrbucket/output/hw5/topfrequent \\\n",
    "    --no-output \\\n",
    "    --no-strict-protocol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "download: s3://hvcemrbucket/output/hw5/topfrequent/_SUCCESS to output/topfrequent/_SUCCESS\n",
      "download: s3://hvcemrbucket/output/hw5/topfrequent/part-00007 to output/topfrequent/part-00007\n",
      "download: s3://hvcemrbucket/output/hw5/topfrequent/part-00006 to output/topfrequent/part-00006\n",
      "download: s3://hvcemrbucket/output/hw5/topfrequent/part-00000 to output/topfrequent/part-00000\n",
      "download: s3://hvcemrbucket/output/hw5/topfrequent/part-00003 to output/topfrequent/part-00003\n",
      "download: s3://hvcemrbucket/output/hw5/topfrequent/part-00004 to output/topfrequent/part-00004\n",
      "download: s3://hvcemrbucket/output/hw5/topfrequent/part-00009 to output/topfrequent/part-00009\n",
      "download: s3://hvcemrbucket/output/hw5/topfrequent/part-00005 to output/topfrequent/part-00005\n",
      "download: s3://hvcemrbucket/output/hw5/topfrequent/part-00001 to output/topfrequent/part-00001\n",
      "download: s3://hvcemrbucket/output/hw5/topfrequent/part-00010 to output/topfrequent/part-00010\n",
      "download: s3://hvcemrbucket/output/hw5/topfrequent/part-00011 to output/topfrequent/part-00011\n",
      "download: s3://hvcemrbucket/output/hw5/topfrequent/part-00002 to output/topfrequent/part-00002\n",
      "download: s3://hvcemrbucket/output/hw5/topfrequent/part-00008 to output/topfrequent/part-00008\n",
      "download: s3://hvcemrbucket/output/hw5/topfrequent/part-00013 to output/topfrequent/part-00013\n",
      "download: s3://hvcemrbucket/output/hw5/topfrequent/part-00012 to output/topfrequent/part-00012\n"
     ]
    }
   ],
   "source": [
    "!rm -fR output/topfrequent\n",
    "!mkdir -p ./output/topfrequent\n",
    "!aws s3 cp --recursive s3://hvcemrbucket/output/hw5/topfrequent ./output/topfrequent\n",
    "# !cat ./output/topfrequent/part* | sort -k2 -n -r | head -10000 > ./output/top10frequenct.txt\n",
    "# !rm -fR output/topfrequent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"the\"\t5490815394\r\n",
      "\"of\"\t3698583299\r\n",
      "\"to\"\t2227866570\r\n",
      "\"in\"\t1421312776\r\n",
      "\"a\"\t1361123022\r\n",
      "\"and\"\t1149577477\r\n",
      "\"that\"\t802921147\r\n",
      "\"is\"\t758328796\r\n",
      "\"be\"\t688707130\r\n",
      "\"as\"\t492170314\r\n"
     ]
    }
   ],
   "source": [
    "!cat ./output/topfrequent/part* | sort -k2nr  > ./output/topwords.txt\n",
    "!rm -fR output/topfrequent\n",
    "!head -10 ./output/topwords.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Most/Least densely appearing words (count/pages_count) sorted in decreasing order of relative frequency (Hint: save to PART-000* and take the head -n 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting density.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile density.py\n",
    "\n",
    "from mrjob.job import MRJob\n",
    "from mrjob.step import MRJobStep\n",
    "import csv, re\n",
    "from mrjob.protocol import RawProtocol, ReprProtocol\n",
    "\n",
    "WORD_RE = re.compile(r\"[\\w']+\")\n",
    "\n",
    "class DensityWords(MRJob):\n",
    "    def steps(self):\n",
    "        return [\n",
    "            MRJobStep(mapper=self.mapper, combiner = self.combiner, reducer = self.reducer)\n",
    "               ]\n",
    "    \n",
    "    def mapper(self, line_no, line):\n",
    "        ngram,occurence,page_count,book_count = line.strip().split('\\t')\n",
    "        words = re.findall(WORD_RE,ngram)\n",
    "        for word in words:\n",
    "            yield word.lower(),(occurence,page_count)\n",
    "        \n",
    "    def combiner(self,word,value):\n",
    "        count =0\n",
    "        pagecount = 0\n",
    "        for cnt, pagecnt in value:\n",
    "            count += int(cnt)\n",
    "            pagecount += int(pagecnt)\n",
    "        yield word, (count,pagecount)\n",
    "        \n",
    "    def reducer(self, word ,value):\n",
    "        count =0\n",
    "        pagecount = 0\n",
    "        for cnt, pagecnt in value:\n",
    "            count += int(cnt)\n",
    "            pagecount += int(pagecnt)\n",
    "        yield word, (1.0*count/pagecount)\n",
    "                          \n",
    "if __name__ == '__main__':\n",
    "    DensityWords.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!python ./density.py  filtered-5Grams/googlebooks-eng-all-5gram-20090715-0-filtered.txt  -q > density.out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most Densely appearing word \n",
      "\"lak\"\t3.072289156626506\n",
      "\"operand\"\t2.353448275862069\n",
      "\"bust\"\t2.3493975903614457\n",
      "\"houseless\"\t2.274891774891775\n",
      "\"gynecological\"\t2.2481536189069424\n",
      "\"denatured\"\t2.1864406779661016\n",
      "\"expiration\"\t2.1568513119533526\n",
      "\"phe\"\t2.0408163265306123\n",
      "\"kiowa\"\t2.0\n",
      "\"apiece\"\t1.9607843137254901\n",
      "\n",
      "Least densely appearing word\n",
      "\"abateth\"\t1.0\n",
      "\"abatement\"\t1.0\n",
      "\"abated\"\t1.0\n",
      "\"abate\"\t1.0\n",
      "\"abased\"\t1.0\n",
      "\"abandons\"\t1.0\n",
      "\"abandoning\"\t1.0\n",
      "\"aback\"\t1.0\n",
      "\"aalborg\"\t1.0\n",
      "\"aahperd\"\t1.0\n"
     ]
    }
   ],
   "source": [
    "!sort -k2 -n -r density.out > density_sorted.out\n",
    "print 'Most Densely appearing word '\n",
    "! head -10 density_sorted.out\n",
    "print '\\nLeast densely appearing word'\n",
    "! tail -10 density_sorted.out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#running job on EMR\n",
    "!python ./density.py \\\n",
    "    -r emr s3://filtered-5grams \\\n",
    "    --output-dir=s3://hvcemrbucket/output/hw5/density \\\n",
    "    --no-output \\\n",
    "    --no-strict-protocol -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 365,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "download: s3://hvcemrbucket/output/hw5/density/_SUCCESS to output/density/_SUCCESS\n",
      "download: s3://hvcemrbucket/output/hw5/density/part-00002 to output/density/part-00002\n",
      "download: s3://hvcemrbucket/output/hw5/density/part-00001 to output/density/part-00001\n",
      "download: s3://hvcemrbucket/output/hw5/density/part-00004 to output/density/part-00004\n",
      "download: s3://hvcemrbucket/output/hw5/density/part-00006 to output/density/part-00006\n",
      "download: s3://hvcemrbucket/output/hw5/density/part-00003 to output/density/part-00003\n",
      "download: s3://hvcemrbucket/output/hw5/density/part-00005 to output/density/part-00005\n",
      "download: s3://hvcemrbucket/output/hw5/density/part-00000 to output/density/part-00000\n"
     ]
    }
   ],
   "source": [
    "!rm -fR output/density\n",
    "!mkdir -p ./output/density\n",
    "!aws s3 cp --recursive s3://hvcemrbucket/output/hw5/density ./output/density"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 366,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most dense words\n",
      "\"xxxx\"\t11.557291666666666\n",
      "\"blah\"\t8.0741599073001158\n",
      "\"nnn\"\t7.5333333333333332\n",
      "\"na\"\t6.2017491314244637\n",
      "\"oooooooooooooooo\"\t4.921875\n",
      "\"nd\"\t4.8543057272352703\n",
      "\"llll\"\t4.5116279069767442\n",
      "\"oooooo\"\t4.169650013358269\n",
      "\"ooooo\"\t3.8586371934672128\n",
      "\"lillelu\"\t3.7624521072796937\n",
      "\n",
      " Least dense words\n",
      "\"zygmunt\"\t1.0\n",
      "\"zygomaticofacial\"\t1.0\n",
      "\"zygomaticotemporal\"\t1.0\n",
      "\"zygosity\"\t1.0\n",
      "\"zylindrischen\"\t1.0\n",
      "\"zymelman\"\t1.0\n",
      "\"zymogens\"\t1.0\n",
      "\"zymophore\"\t1.0\n",
      "\"zymosan\"\t1.0\n",
      "\"zymosis\"\t1.0\n"
     ]
    }
   ],
   "source": [
    "!rm ./output/density_emr.out\n",
    "!cat ./output/density/part* | sort -k2nr  > ./output/density_emr.out\n",
    "!rm -fR output/density\n",
    "print \"Most dense words\"\n",
    "!head -10 ./output/density_emr.out\n",
    "print \"\\n Least dense words\"\n",
    "!tail -10 ./output/density_emr.out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color: blue\"> Distribution of 5-gram sizes (character length). E.g., count (using the count field) up how many times a 5-gram of 50 characters shows up. Plot the data graphically using a histogram.</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ngram_distribution.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ngram_distribution.py\n",
    "\n",
    "from mrjob.job import MRJob\n",
    "from mrjob.step import MRJobStep\n",
    "import csv, re\n",
    "from mrjob.protocol import RawProtocol, ReprProtocol\n",
    "\n",
    "WORD_RE = re.compile(r\"[\\w']+\")\n",
    "\n",
    "class LengthDistribution(MRJob):\n",
    "    count = 0\n",
    "    reducer_total = 0\n",
    "    def steps(self):\n",
    "        return [\n",
    "            MRJobStep(mapper=self.mapper, combiner = self.combiner, reducer = self.reducer)\n",
    "               ]\n",
    "    \n",
    "    def mapper(self, line_no, line):\n",
    "        ngram,occurence,page_count,book_count = line.strip().split('\\t')\n",
    "        yield len(ngram),1\n",
    "        \n",
    "    def combiner(self,key,value):\n",
    "        yield key, sum(value)\n",
    "        \n",
    "    def reducer(self, key ,value):\n",
    "        yield key,sum(value)\n",
    "                          \n",
    "if __name__ == '__main__':\n",
    "    LengthDistribution.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 368,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\t1\r\n",
      "11\t1\r\n",
      "12\t3\r\n",
      "13\t22\r\n",
      "14\t94\r\n",
      "15\t309\r\n",
      "16\t1112\r\n",
      "17\t2859\r\n",
      "18\t5853\r\n",
      "19\t10200\r\n",
      "20\t15479\r\n",
      "21\t20108\r\n",
      "22\t24625\r\n",
      "23\t27333\r\n",
      "24\t28052\r\n",
      "25\t27690\r\n",
      "26\t25991\r\n",
      "27\t23405\r\n",
      "28\t20587\r\n",
      "29\t17257\r\n",
      "30\t14428\r\n",
      "31\t11340\r\n",
      "32\t9061\r\n",
      "33\t6950\r\n",
      "34\t5152\r\n",
      "35\t3871\r\n",
      "36\t2868\r\n",
      "37\t2027\r\n",
      "38\t1516\r\n",
      "39\t1027\r\n",
      "40\t756\r\n",
      "41\t476\r\n",
      "42\t337\r\n",
      "43\t263\r\n",
      "44\t195\r\n",
      "45\t113\r\n",
      "46\t87\r\n",
      "47\t51\r\n",
      "48\t33\r\n",
      "49\t31\r\n",
      "50\t13\r\n",
      "51\t10\r\n",
      "52\t9\r\n",
      "53\t8\r\n",
      "54\t2\r\n",
      "55\t4\r\n",
      "57\t2\r\n",
      "58\t2\r\n",
      "9\t1\r\n"
     ]
    }
   ],
   "source": [
    "!python ./ngram_distribution.py  filtered-5Grams/googlebooks-eng-all-5gram-20090715-0-filtered.txt  -q "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 369,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!python ./ngram_distribution.py \\\n",
    "-r emr s3://filtered-5grams \\\n",
    "    --output-dir=s3://hvcemrbucket/output/hw5/distribution \\\n",
    "    --no-output \\\n",
    "    --no-strict-protocol -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "download: s3://hvcemrbucket/output/hw5/distribution/_SUCCESS to output/distribution/_SUCCESS\n",
      "download: s3://hvcemrbucket/output/hw5/distribution/part-00006 to output/distribution/part-00006\n",
      "download: s3://hvcemrbucket/output/hw5/distribution/part-00001 to output/distribution/part-00001\n",
      "download: s3://hvcemrbucket/output/hw5/distribution/part-00000 to output/distribution/part-00000\n",
      "download: s3://hvcemrbucket/output/hw5/distribution/part-00003 to output/distribution/part-00003\n",
      "download: s3://hvcemrbucket/output/hw5/distribution/part-00004 to output/distribution/part-00004\n",
      "download: s3://hvcemrbucket/output/hw5/distribution/part-00005 to output/distribution/part-00005\n",
      "download: s3://hvcemrbucket/output/hw5/distribution/part-00002 to output/distribution/part-00002\n",
      "rm: ./output/distribution_all.out: No such file or directory\n"
     ]
    }
   ],
   "source": [
    "#get top 1000 closest pairs from jaccard similarity\n",
    "!rm -fR output/distribution\n",
    "!mkdir -p ./output/distribution\n",
    "!aws s3 cp --recursive s3://hvcemrbucket/output/hw5/distribution ./output/distribution\n",
    "!rm ./output/distribution_all.out\n",
    "!cat ./output/distribution/part*   > ./output/distribution_all.out\n",
    "!rm -fR output/distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hetal/anaconda/lib/python2.7/site-packages/ipykernel/__main__.py:8: FutureWarning: sort(columns=....) is deprecated, use sort_values(by=.....)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.text.Text at 0x127aed790>"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA8oAAAIBCAYAAACRN0b3AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nA/M+QuK+OEjbAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x127a9ffd0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.rcParams['figure.figsize'] = 16, 8  # plotsize \n",
    "\n",
    "df = pd.read_csv('./output/distribution_all.out',sep='\\t',header=None)\n",
    "df.columns = ['length','frequency']\n",
    "df = df.sort('length')\n",
    "df = df.set_index('length')\n",
    "my_plot = df.plot(kind='bar',legend=None,title=\"5-gram character length distribution\")\n",
    "my_plot.set_xlabel(\"5-gram length\")\n",
    "my_plot.set_ylabel(\"frequency\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### HW 5.4  (over 2Gig of Data)\n",
    "In this part of the assignment we will focus on developing methods\n",
    "for detecting synonyms, using the Google 5-grams dataset. To accomplish\n",
    "this you must script two main tasks using MRJob:\n",
    "\n",
    "(1) Build stripes of word co-ocurrence for the top 10,000 using the words ranked from 9001,-10,000 as a basis\n",
    "most frequently appearing words across the entire set of 5-grams,\n",
    "and output to a file in your bucket on s3 (bigram analysis, though the words are non-contiguous).\n",
    "\n",
    "(2) Using two (symmetric) comparison methods of your choice \n",
    "(e.g., correlations, distances, similarities), pairwise compare \n",
    "all stripes (vectors), and output to a file in your bucket on s3.\n",
    "\n",
    "==Design notes for (1)==\n",
    "For this task you will be able to modify the pattern we used in HW 3.2\n",
    "(feel free to use the solution as reference). To total the word counts \n",
    "across the 5-grams, output the support from the mappers using the total \n",
    "order inversion pattern:\n",
    "\n",
    "<*word,count>\n",
    "\n",
    "to ensure that the support arrives before the cooccurrences.\n",
    "\n",
    "In addition to ensuring the determination of the total word counts,\n",
    "the mapper must also output co-occurrence counts for the pairs of\n",
    "words inside of each 5-gram. Treat these words as a basket,\n",
    "as we have in HW 3, but count all stripes or pairs in both orders,\n",
    "i.e., count both orderings: (word1,word2), and (word2,word1), to preserve\n",
    "symmetry in our output for (2).\n",
    "\n",
    "==Design notes for (2)==\n",
    "For this task you will have to determine a method of comparison.\n",
    "Here are a few that you might consider:\n",
    "\n",
    "- Jaccard\n",
    "- Cosine similarity\n",
    "- Spearman correlation\n",
    "- Euclidean distance\n",
    "- Taxicab (Manhattan) distance\n",
    "- Shortest path graph distance (a graph, because our data is symmetric!)\n",
    "- Pearson correlation\n",
    "- Kendall correlation\n",
    "...\n",
    "\n",
    "However, be cautioned that some comparison methods are more difficult to\n",
    "parallelize than others, and do not perform more associations than is necessary, \n",
    "since your choice of association will be symmetric.\n",
    "\n",
    "Please use the inverted index (discussed in live session #5) based pattern to compute the pairwise (term-by-term) similarity matrix. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "upload: ./topwords_touse.out to s3://hvcemrbucket/input/hw5/topwords_touse.out\r\n"
     ]
    }
   ],
   "source": [
    "! rm topwords_touse.out\n",
    "!cat ./output/topwords.txt | sed -n 9001,10000p > topwords_touse.out\n",
    "!aws s3 cp topwords_touse.out s3://hvcemrbucket/input/hw5/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 354,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting stripes.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile stripes.py\n",
    "\n",
    "from mrjob.job import MRJob\n",
    "from mrjob.step import MRStep\n",
    "import csv, re, string\n",
    "import urllib2\n",
    "from mrjob.protocol import RawProtocol, ReprProtocol\n",
    "\n",
    "WORD_RE = re.compile(r\"[\\w']+\")\n",
    "\n",
    "class Stripes(MRJob):\n",
    "    topwords={}\n",
    "    def steps(self):\n",
    "        return [\n",
    "            MRStep(mapper_init=self.mapper_init, mapper=self.mapper, combiner= self.combiner, reducer=self.reducer)\n",
    "               ]\n",
    "    \n",
    "    def mapper_init(self):\n",
    "        top_words = 'https://s3-us-west-1.amazonaws.com/hvcemrbucket/input/hw5/topwords_touse.out'\n",
    "        a = urllib2.urlopen(top_words)\n",
    "        data= a.read().strip().split('\\n')\n",
    "        top_touse=[]\n",
    "        for row in data:\n",
    "            key = row.strip().split('\\t')\n",
    "            top_touse.append(key[0].replace('\"',''))\n",
    "        self.topwords=set(top_touse) \n",
    "#         with open('topwords.out') as f:\n",
    "#             self.topwords = {k for line in f for (k, v) in (line.replace('\"', '').strip().split('\\t'),)}\n",
    "    \n",
    "    def mapper(self, line_no, line):\n",
    "        words_stripe={}\n",
    "        ngram,occurence,page_count,book_count = line.strip().split('\\t')\n",
    "        ngram_strip =ngram.translate(None, string.punctuation)\n",
    "        words = re.findall(WORD_RE,ngram_strip.lower())\n",
    "        for term in words:\n",
    "            if term not in self.topwords:\n",
    "                continue\n",
    "            if term not in words_stripe:\n",
    "                words_stripe[term]={}  \n",
    "            for word in words:\n",
    "                if word in self.topwords and term != word:\n",
    "                    if word not in words_stripe[term]:\n",
    "                        words_stripe[term][word] = int(occurence)\n",
    "                    else:\n",
    "                        words_stripe[term][word] += int(occurence)\n",
    "        for term, neighbors in words_stripe.iteritems():\n",
    "            if len(neighbors) != 0:\n",
    "                yield term, neighbors\n",
    "    \n",
    "       \n",
    "    def combiner(self,term,neighbors):\n",
    "        words_stripe ={}\n",
    "        words_stripe[term]={}\n",
    "        for v in neighbors:\n",
    "            for key in v:\n",
    "                if key not in words_stripe[term]:\n",
    "                    words_stripe[term][key] = v[key]\n",
    "                else :\n",
    "                    words_stripe[term][key] += v[key]\n",
    "        for term, neighbors in words_stripe.iteritems():\n",
    "            yield term, neighbors\n",
    "    \n",
    "    def reducer(self,term,neighbors):\n",
    "        words_stripe ={}\n",
    "        for v in neighbors:\n",
    "            for key in v:\n",
    "                if key not in words_stripe:\n",
    "                    words_stripe[key] = 0\n",
    "                words_stripe[key] += v[key]\n",
    "        yield term, words_stripe\n",
    "                   \n",
    "if __name__ == '__main__':\n",
    "    Stripes.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 357,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"alternate\"\t{\"viewing\": 82}\r\n",
      "\"amidst\"\t{\"tumult\": 80, \"restless\": 43}\r\n",
      "\"ammonium\"\t{\"hydroxide\": 78}\r\n",
      "\"anemia\"\t{\"pernicious\": 58}\r\n",
      "\"annum\"\t{\"thereon\": 69}\r\n",
      "\"approximated\"\t{\"subcutaneous\": 120}\r\n",
      "\"architectural\"\t{\"decoration\": 46}\r\n",
      "\"articular\"\t{\"cartilage\": 51}\r\n",
      "\"authoritative\"\t{\"interpreter\": 50}\r\n",
      "\"balcony\"\t{\"overlooking\": 139}\r\n",
      "\"bottles\"\t{\"necks\": 65}\r\n",
      "\"brightest\"\t{\"diamond\": 71}\r\n",
      "\"canons\"\t{\"commonest\": 43}\r\n",
      "\"careless\"\t{\"hasty\": 58}\r\n",
      "\"cartilage\"\t{\"articular\": 51, \"localized\": 51}\r\n",
      "\"ce\"\t{\"qui\": 48}\r\n",
      "\"commence\"\t{\"qui\": 63, \"palestinian\": 62}\r\n",
      "\"commonest\"\t{\"canons\": 43}\r\n",
      "\"commonplace\"\t{\"feathers\": 84}\r\n",
      "\"complexion\"\t{\"darker\": 86}\r\n",
      "\"contradictory\"\t{\"predicate\": 171}\r\n",
      "\"conveying\"\t{\"pipes\": 155}\r\n",
      "\"dame\"\t{\"habitation\": 66}\r\n",
      "\"darker\"\t{\"complexion\": 86}\r\n",
      "\"darkest\"\t{\"superstition\": 110}\r\n",
      "\"decoration\"\t{\"architectural\": 46}\r\n",
      "\"deliberations\"\t{\"trent\": 47}\r\n",
      "\"diamond\"\t{\"brightest\": 71}\r\n",
      "\"discomfort\"\t{\"localized\": 43}\r\n",
      "\"dividend\"\t{\"shareholders\": 43}\r\n",
      "\"dumb\"\t{\"pretending\": 69}\r\n",
      "\"endowment\"\t{\"unstable\": 84}\r\n",
      "\"est\"\t{\"qui\": 83}\r\n",
      "\"establishments\"\t{\"sanitary\": 49}\r\n",
      "\"feathers\"\t{\"commonplace\": 84}\r\n",
      "\"flexor\"\t{\"sheath\": 203}\r\n",
      "\"flocks\"\t{\"herds\": 969}\r\n",
      "\"flourish\"\t{\"jungle\": 137}\r\n",
      "\"fossil\"\t{\"shells\": 103}\r\n",
      "\"habitation\"\t{\"dame\": 66}\r\n",
      "\"hasty\"\t{\"careless\": 58, \"impatience\": 41}\r\n",
      "\"herds\"\t{\"flocks\": 969, \"restless\": 43}\r\n",
      "\"humiliation\"\t{\"intolerable\": 127, \"rejoiced\": 132}\r\n",
      "\"hydroxide\"\t{\"soda\": 42, \"ammonium\": 78}\r\n",
      "\"impatience\"\t{\"hasty\": 41}\r\n",
      "\"indiana\"\t{\"linguistics\": 128}\r\n",
      "\"inspector\"\t{\"sanitary\": 46}\r\n",
      "\"interpreter\"\t{\"authoritative\": 50}\r\n",
      "\"intolerable\"\t{\"humiliation\": 127}\r\n",
      "\"irresistible\"\t{\"speedily\": 44}\r\n",
      "\"jones\"\t{\"summon\": 83}\r\n",
      "\"jungle\"\t{\"flourish\": 137}\r\n",
      "\"laden\"\t{\"spoils\": 67}\r\n",
      "\"linguistics\"\t{\"indiana\": 128}\r\n",
      "\"localized\"\t{\"discomfort\": 43, \"cartilage\": 51}\r\n",
      "\"magnificence\"\t{\"sketches\": 66}\r\n",
      "\"matthew\"\t{\"sayings\": 76}\r\n",
      "\"meridian\"\t{\"zenith\": 141}\r\n",
      "\"necks\"\t{\"bottles\": 65}\r\n",
      "\"operative\"\t{\"wholesale\": 43}\r\n",
      "\"overlooking\"\t{\"balcony\": 139}\r\n",
      "\"palestinian\"\t{\"commence\": 62}\r\n",
      "\"peritoneal\"\t{\"sac\": 77}\r\n",
      "\"pernicious\"\t{\"anemia\": 58}\r\n",
      "\"pink\"\t{\"replacing\": 47, \"wax\": 411}\r\n",
      "\"pipes\"\t{\"conveying\": 155}\r\n",
      "\"pitched\"\t{\"tents\": 123}\r\n",
      "\"polished\"\t{\"shells\": 59}\r\n",
      "\"predicate\"\t{\"contradictory\": 171}\r\n",
      "\"pretending\"\t{\"dumb\": 69}\r\n",
      "\"qui\"\t{\"est\": 83, \"commence\": 63, \"ce\": 48}\r\n",
      "\"rejoiced\"\t{\"humiliation\": 132}\r\n",
      "\"relaxed\"\t{\"vigilance\": 145}\r\n",
      "\"replacing\"\t{\"pink\": 47}\r\n",
      "\"resembling\"\t{\"shells\": 107}\r\n",
      "\"restless\"\t{\"amidst\": 43, \"herds\": 43}\r\n",
      "\"sac\"\t{\"uterine\": 48, \"peritoneal\": 77}\r\n",
      "\"sanitary\"\t{\"inspector\": 46, \"establishments\": 49}\r\n",
      "\"sayings\"\t{\"matthew\": 76}\r\n",
      "\"shareholders\"\t{\"dividend\": 43}\r\n",
      "\"sheath\"\t{\"flexor\": 203}\r\n",
      "\"shells\"\t{\"resembling\": 107, \"polished\": 59, \"fossil\": 103}\r\n",
      "\"sketches\"\t{\"magnificence\": 66}\r\n",
      "\"soda\"\t{\"hydroxide\": 42}\r\n",
      "\"speedily\"\t{\"irresistible\": 44}\r\n",
      "\"spoils\"\t{\"laden\": 67}\r\n",
      "\"subcutaneous\"\t{\"approximated\": 120}\r\n",
      "\"summon\"\t{\"jones\": 83}\r\n",
      "\"superstition\"\t{\"darkest\": 110}\r\n",
      "\"telescope\"\t{\"viewing\": 93}\r\n",
      "\"tents\"\t{\"pitched\": 123}\r\n",
      "\"thereon\"\t{\"annum\": 69}\r\n",
      "\"trent\"\t{\"deliberations\": 47}\r\n",
      "\"tumult\"\t{\"amidst\": 80}\r\n",
      "\"unstable\"\t{\"endowment\": 84}\r\n",
      "\"uterine\"\t{\"sac\": 48}\r\n",
      "\"viewing\"\t{\"alternate\": 82, \"telescope\": 93}\r\n",
      "\"vigilance\"\t{\"relaxed\": 145}\r\n",
      "\"wax\"\t{\"pink\": 411}\r\n",
      "\"wholesale\"\t{\"operative\": 43}\r\n",
      "\"zenith\"\t{\"meridian\": 141}\r\n"
     ]
    }
   ],
   "source": [
    "!python ./stripes.py  filtered-5Grams/googlebooks-eng-all-5gram-20090715-0-filtered.txt -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 356,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Got unexpected keyword arguments: ssh_tunnel\n",
      "using configs in /Users/hetal/.mrjob.conf\n",
      "creating new scratch bucket mrjob-f7f5fce3ffcc49a1\n",
      "using s3://mrjob-f7f5fce3ffcc49a1/tmp/ as our scratch dir on S3\n",
      "creating tmp directory /var/folders/91/cjfxt7ys6958qll6vjtgwwfw0000gn/T/stripes.hetal.20160215.075055.806501\n",
      "writing master bootstrap script to /var/folders/91/cjfxt7ys6958qll6vjtgwwfw0000gn/T/stripes.hetal.20160215.075055.806501/b.py\n",
      "creating S3 bucket 'mrjob-f7f5fce3ffcc49a1' to use as scratch space\n",
      "Copying non-input files into s3://mrjob-f7f5fce3ffcc49a1/tmp/stripes.hetal.20160215.075055.806501/files/\n",
      "Waiting 5.0s for S3 eventual consistency\n",
      "Creating Elastic MapReduce job flow\n",
      "Job flow created with ID: j-2WCZ4WQW9O9BP\n",
      "Created new job flow j-2WCZ4WQW9O9BP\n",
      "Job launched 30.8s ago, status STARTING: Provisioning Amazon EC2 capacity\n",
      "Job launched 62.3s ago, status STARTING: Provisioning Amazon EC2 capacity\n",
      "Job launched 93.2s ago, status STARTING: Provisioning Amazon EC2 capacity\n",
      "Job launched 124.6s ago, status STARTING: Provisioning Amazon EC2 capacity\n",
      "Job launched 155.6s ago, status STARTING: Provisioning Amazon EC2 capacity\n",
      "Job launched 187.0s ago, status STARTING: Configuring cluster software\n",
      "Job launched 217.8s ago, status STARTING: Configuring cluster software\n",
      "Job launched 249.2s ago, status RUNNING: Running step (stripes.hetal.20160215.075055.806501: Step 1 of 1)\n",
      "Job launched 280.0s ago, status RUNNING: Running step (stripes.hetal.20160215.075055.806501: Step 1 of 1)\n",
      "Job launched 311.3s ago, status RUNNING: Running step (stripes.hetal.20160215.075055.806501: Step 1 of 1)\n",
      "Job launched 342.2s ago, status RUNNING: Running step (stripes.hetal.20160215.075055.806501: Step 1 of 1)\n",
      "Job launched 373.4s ago, status RUNNING: Running step (stripes.hetal.20160215.075055.806501: Step 1 of 1)\n",
      "Job launched 404.3s ago, status RUNNING: Running step (stripes.hetal.20160215.075055.806501: Step 1 of 1)\n",
      "Job launched 435.7s ago, status RUNNING: Running step (stripes.hetal.20160215.075055.806501: Step 1 of 1)\n",
      "Job launched 466.5s ago, status RUNNING: Running step (stripes.hetal.20160215.075055.806501: Step 1 of 1)\n",
      "Job launched 497.8s ago, status RUNNING: Running step (stripes.hetal.20160215.075055.806501: Step 1 of 1)\n",
      "Job launched 528.6s ago, status RUNNING: Running step (stripes.hetal.20160215.075055.806501: Step 1 of 1)\n",
      "Job launched 560.0s ago, status RUNNING: Running step (stripes.hetal.20160215.075055.806501: Step 1 of 1)\n",
      "Job completed.\n",
      "Running time was 325.0s (not counting time spent waiting for the EC2 instances)\n",
      "Fetching counters from S3...\n",
      "Waiting 5.0s for S3 eventual consistency\n",
      "Counters from step 1:\n",
      "  File Input Format Counters :\n",
      "    Bytes Read: 2156069116\n",
      "  File Output Format Counters :\n",
      "    Bytes Written: 208957\n",
      "  FileSystemCounters:\n",
      "    FILE_BYTES_READ: 281362\n",
      "    FILE_BYTES_WRITTEN: 6233480\n",
      "    HDFS_BYTES_READ: 23640\n",
      "    S3_BYTES_READ: 2156069116\n",
      "    S3_BYTES_WRITTEN: 208957\n",
      "  Job Counters :\n",
      "    Launched map tasks: 194\n",
      "    Launched reduce tasks: 10\n",
      "    Rack-local map tasks: 192\n",
      "    SLOTS_MILLIS_MAPS: 2651330\n",
      "    SLOTS_MILLIS_REDUCES: 975908\n",
      "    Total time spent by all maps waiting after reserving slots (ms): 0\n",
      "    Total time spent by all reduces waiting after reserving slots (ms): 0\n",
      "  Map-Reduce Framework:\n",
      "    CPU time spent (ms): 857660\n",
      "    Combine input records: 25096\n",
      "    Combine output records: 22002\n",
      "    Map input bytes: 2156069116\n",
      "    Map input records: 58682266\n",
      "    Map output bytes: 688842\n",
      "    Map output materialized bytes: 640918\n",
      "    Map output records: 25096\n",
      "    Physical memory (bytes) snapshot: 120500846592\n",
      "    Reduce input groups: 991\n",
      "    Reduce input records: 22002\n",
      "    Reduce output records: 991\n",
      "    Reduce shuffle bytes: 640918\n",
      "    SPLIT_RAW_BYTES: 23640\n",
      "    Spilled Records: 44004\n",
      "    Total committed heap usage (bytes): 121330728960\n",
      "    Virtual memory (bytes) snapshot: 260901158912\n",
      "removing tmp directory /var/folders/91/cjfxt7ys6958qll6vjtgwwfw0000gn/T/stripes.hetal.20160215.075055.806501\n",
      "Removing all files in s3://mrjob-f7f5fce3ffcc49a1/tmp/stripes.hetal.20160215.075055.806501/\n",
      "Removing all files in s3://hvcemrbucket/logs/j-2WCZ4WQW9O9BP/\n",
      "Terminating job flow: j-2WCZ4WQW9O9BP\n"
     ]
    }
   ],
   "source": [
    "!python ./stripes.py  \\\n",
    "-r emr s3://filtered-5grams \\\n",
    "--output-dir=s3://hvcemrbucket/output/hw5/stripes \\\n",
    "--no-output \\\n",
    "--no-strict-protocol "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting jaccard.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile jaccard.py\n",
    "\n",
    "from mrjob.job import MRJob\n",
    "from mrjob.step import MRStep\n",
    "import csv, re, string\n",
    "from mrjob.protocol import RawProtocol, ReprProtocol\n",
    "\n",
    "WORD_RE = re.compile(r\"[\\w']+\")\n",
    "\n",
    "class Jaccard(MRJob):\n",
    "    global_doc_dict = {}\n",
    "    def steps(self):\n",
    "        return [\n",
    "            MRStep(mapper=self.mapper , reducer= self.reducer, \n",
    "                   jobconf={\n",
    "                    \"mapred.map.tasks\":16,\n",
    "                    \"mapred.reduce.tasks\":8\n",
    "                    }),\n",
    "            MRStep(mapper=self.mapper2 ,combiner=self.combiner2, reducer=self.reducer2,\n",
    "                   jobconf={\n",
    "                    \"mapred.map.tasks\":8,\n",
    "                    \"mapred.reduce.tasks\":4\n",
    "                    }\n",
    "                  ),\n",
    "             MRStep(reducer=self.jaccard_cal, \n",
    "                    jobconf={\n",
    "                    \"mapred.map.tasks\":4,\n",
    "                    \"mapred.reduce.tasks\":1\n",
    "                    })\n",
    "            \n",
    "               ]\n",
    "    \n",
    "    def mapper(self, _, line):\n",
    "        key,terms = line.strip().split('\\t')\n",
    "        docs = eval(terms).keys()\n",
    "        for doc in docs:\n",
    "            yield doc,key.replace('\"', '')\n",
    "    \n",
    "    def reducer(self,key,value):\n",
    "        doc_list ={}\n",
    "        for v in value:\n",
    "            doc_list[v]=1\n",
    "        yield key, doc_list.keys()\n",
    "        \n",
    "    def mapper2(self,key,value):\n",
    "        doc_list = list(value)\n",
    "        for i in range(0,len(doc_list)):\n",
    "            starkey = '*' + doc_list[i]\n",
    "            yield (starkey, doc_list[i]),1\n",
    "            for j in range(i+1, len(doc_list)):\n",
    "                yield(doc_list[i],doc_list[j]),1\n",
    "    \n",
    "    def combiner2(self,key,value):\n",
    "        yield key,sum(value)\n",
    "    \n",
    "    def reducer2(self,key,value):\n",
    "        yield key,sum(value)\n",
    "    \n",
    "    def jaccard_cal(self,key,value):\n",
    "        docA,docB = key\n",
    "\n",
    "        if docA.startswith('*'): #|doc|\n",
    "            self.global_doc_dict[docB] = sum(value)\n",
    "        else:  #at this point we have all the |doc|\n",
    "            ab = sum(value)\n",
    "            calc = 1.0*ab / (self.global_doc_dict[docA] + self.global_doc_dict[docB] - ab)\n",
    "            yield (docA,docB), calc\n",
    " \n",
    "                   \n",
    "if __name__ == '__main__':\n",
    "    Jaccard.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!python ./jaccard.py -r hadoop inv_index.out -q >jaccard_similarity.out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Got unexpected keyword arguments: ssh_tunnel\n",
      "using configs in /Users/hetal/.mrjob.conf\n",
      "using existing scratch bucket mrjob-f7f5fce3ffcc49a1\n",
      "using s3://mrjob-f7f5fce3ffcc49a1/tmp/ as our scratch dir on S3\n",
      "creating tmp directory /var/folders/91/cjfxt7ys6958qll6vjtgwwfw0000gn/T/jaccard.hetal.20160215.174037.268622\n",
      "writing master bootstrap script to /var/folders/91/cjfxt7ys6958qll6vjtgwwfw0000gn/T/jaccard.hetal.20160215.174037.268622/b.py\n",
      "Copying non-input files into s3://mrjob-f7f5fce3ffcc49a1/tmp/jaccard.hetal.20160215.174037.268622/files/\n",
      "Waiting 5.0s for S3 eventual consistency\n",
      "Creating Elastic MapReduce job flow\n",
      "Job flow created with ID: j-PNXRGWIBHK15\n",
      "Created new job flow j-PNXRGWIBHK15\n",
      "Job launched 30.9s ago, status STARTING: Provisioning Amazon EC2 capacity\n",
      "Job launched 62.2s ago, status STARTING: Provisioning Amazon EC2 capacity\n",
      "Job launched 93.1s ago, status STARTING: Provisioning Amazon EC2 capacity\n",
      "Job launched 124.4s ago, status STARTING: Provisioning Amazon EC2 capacity\n",
      "Job launched 155.2s ago, status STARTING: Provisioning Amazon EC2 capacity\n",
      "Job launched 186.7s ago, status STARTING: Configuring cluster software\n",
      "Job launched 217.5s ago, status STARTING: Configuring cluster software\n",
      "Job launched 248.9s ago, status BOOTSTRAPPING: Running bootstrap actions\n",
      "Job launched 279.7s ago, status RUNNING: Running step (jaccard.hetal.20160215.174037.268622: Step 1 of 3)\n",
      "Job launched 311.1s ago, status RUNNING: Running step (jaccard.hetal.20160215.174037.268622: Step 1 of 3)\n",
      "Job launched 341.9s ago, status RUNNING: Running step (jaccard.hetal.20160215.174037.268622: Step 1 of 3)\n",
      "Job launched 373.2s ago, status RUNNING: Running step (jaccard.hetal.20160215.174037.268622: Step 2 of 3)\n",
      "Job launched 404.1s ago, status RUNNING: Running step (jaccard.hetal.20160215.174037.268622: Step 2 of 3)\n",
      "Job launched 435.5s ago, status RUNNING: Running step (jaccard.hetal.20160215.174037.268622: Step 3 of 3)\n",
      "Job launched 466.3s ago, status RUNNING: Running step (jaccard.hetal.20160215.174037.268622: Step 3 of 3)\n",
      "Job launched 497.7s ago, status RUNNING: Running step (jaccard.hetal.20160215.174037.268622: Step 3 of 3)\n",
      "Job completed.\n",
      "Running time was 202.0s (not counting time spent waiting for the EC2 instances)\n",
      "Fetching counters from S3...\n",
      "Waiting 5.0s for S3 eventual consistency\n",
      "Counters from step 1:\n",
      "  File Input Format Counters :\n",
      "    Bytes Read: 208957\n",
      "  File Output Format Counters :\n",
      "    Bytes Written: 155571\n",
      "  FileSystemCounters:\n",
      "    FILE_BYTES_READ: 132952\n",
      "    FILE_BYTES_WRITTEN: 508592\n",
      "    HDFS_BYTES_READ: 700\n",
      "    HDFS_BYTES_WRITTEN: 155571\n",
      "    S3_BYTES_READ: 208957\n",
      "  Job Counters :\n",
      "    Launched map tasks: 7\n",
      "    Launched reduce tasks: 2\n",
      "    Rack-local map tasks: 7\n",
      "    SLOTS_MILLIS_MAPS: 79585\n",
      "    SLOTS_MILLIS_REDUCES: 29554\n",
      "    Total time spent by all maps waiting after reserving slots (ms): 0\n",
      "    Total time spent by all reduces waiting after reserving slots (ms): 0\n",
      "  Map-Reduce Framework:\n",
      "    CPU time spent (ms): 24400\n",
      "    Combine input records: 0\n",
      "    Combine output records: 0\n",
      "    Map input bytes: 208957\n",
      "    Map input records: 991\n",
      "    Map output bytes: 263196\n",
      "    Map output materialized bytes: 136081\n",
      "    Map output records: 12138\n",
      "    Physical memory (bytes) snapshot: 2471878656\n",
      "    Reduce input groups: 991\n",
      "    Reduce input records: 12138\n",
      "    Reduce output records: 991\n",
      "    Reduce shuffle bytes: 136081\n",
      "    SPLIT_RAW_BYTES: 700\n",
      "    Spilled Records: 24276\n",
      "    Total committed heap usage (bytes): 2581069824\n",
      "    Virtual memory (bytes) snapshot: 11939233792\n",
      "Counters from step 2:\n",
      "  File Input Format Counters :\n",
      "    Bytes Read: 208861\n",
      "  File Output Format Counters :\n",
      "    Bytes Written: 2081143\n",
      "  FileSystemCounters:\n",
      "    FILE_BYTES_READ: 798585\n",
      "    FILE_BYTES_WRITTEN: 1849511\n",
      "    HDFS_BYTES_READ: 209457\n",
      "    HDFS_BYTES_WRITTEN: 2081143\n",
      "  Job Counters :\n",
      "    Data-local map tasks: 4\n",
      "    Launched map tasks: 4\n",
      "    Launched reduce tasks: 2\n",
      "    SLOTS_MILLIS_MAPS: 43386\n",
      "    SLOTS_MILLIS_REDUCES: 42135\n",
      "    Total time spent by all maps waiting after reserving slots (ms): 0\n",
      "    Total time spent by all reduces waiting after reserving slots (ms): 0\n",
      "  Map-Reduce Framework:\n",
      "    CPU time spent (ms): 17780\n",
      "    Combine input records: 101088\n",
      "    Combine output records: 89263\n",
      "    Map input bytes: 155571\n",
      "    Map input records: 991\n",
      "    Map output bytes: 2694551\n",
      "    Map output materialized bytes: 889519\n",
      "    Map output records: 101088\n",
      "    Physical memory (bytes) snapshot: 1474543616\n",
      "    Reduce input groups: 78308\n",
      "    Reduce input records: 89263\n",
      "    Reduce output records: 78308\n",
      "    Reduce shuffle bytes: 889519\n",
      "    SPLIT_RAW_BYTES: 596\n",
      "    Spilled Records: 178526\n",
      "    Total committed heap usage (bytes): 1503657984\n",
      "    Virtual memory (bytes) snapshot: 8047108096\n",
      "Counters from step 3:\n",
      "  File Input Format Counters :\n",
      "    Bytes Read: 3073461\n",
      "  File Output Format Counters :\n",
      "    Bytes Written: 3454380\n",
      "  FileSystemCounters:\n",
      "    FILE_BYTES_READ: 735811\n",
      "    FILE_BYTES_WRITTEN: 2182691\n",
      "    HDFS_BYTES_READ: 3077186\n",
      "    S3_BYTES_WRITTEN: 3454380\n",
      "  Job Counters :\n",
      "    Data-local map tasks: 16\n",
      "    Launched map tasks: 25\n",
      "    Launched reduce tasks: 1\n",
      "    Rack-local map tasks: 9\n",
      "    SLOTS_MILLIS_MAPS: 198788\n",
      "    SLOTS_MILLIS_REDUCES: 25029\n",
      "    Total time spent by all maps waiting after reserving slots (ms): 0\n",
      "    Total time spent by all reduces waiting after reserving slots (ms): 0\n",
      "  Map-Reduce Framework:\n",
      "    CPU time spent (ms): 28510\n",
      "    Combine input records: 0\n",
      "    Combine output records: 0\n",
      "    Map input bytes: 2081143\n",
      "    Map input records: 78308\n",
      "    Map output bytes: 2081143\n",
      "    Map output materialized bytes: 756666\n",
      "    Map output records: 78308\n",
      "    Physical memory (bytes) snapshot: 10651525120\n",
      "    Reduce input groups: 78308\n",
      "    Reduce input records: 78308\n",
      "    Reduce output records: 77317\n",
      "    Reduce shuffle bytes: 756666\n",
      "    SPLIT_RAW_BYTES: 3725\n",
      "    Spilled Records: 156616\n",
      "    Total committed heap usage (bytes): 10927210496\n",
      "    Virtual memory (bytes) snapshot: 34466938880\n",
      "removing tmp directory /var/folders/91/cjfxt7ys6958qll6vjtgwwfw0000gn/T/jaccard.hetal.20160215.174037.268622\n",
      "Removing all files in s3://mrjob-f7f5fce3ffcc49a1/tmp/jaccard.hetal.20160215.174037.268622/\n",
      "Removing all files in s3://hvcemrbucket/logs/j-PNXRGWIBHK15/\n",
      "Terminating job flow: j-PNXRGWIBHK15\n"
     ]
    }
   ],
   "source": [
    "!python ./jaccard.py  \\\n",
    "-r emr s3://hvcemrbucket/output/hw5/stripes/ \\\n",
    "--output-dir=s3://hvcemrbucket/output/hw5/jaccard \\\n",
    "--no-output \\\n",
    "--no-strict-protocol \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting cosine_similarity.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile cosine_similarity.py\n",
    "\n",
    "from mrjob.job import MRJob\n",
    "from mrjob.step import MRStep\n",
    "import csv, re, string, math\n",
    "from mrjob.protocol import RawProtocol, ReprProtocol\n",
    "\n",
    "WORD_RE = re.compile(r\"[\\w']+\")\n",
    "\n",
    "class Cosine(MRJob):\n",
    "    global_doc_dict = {}\n",
    "    def steps(self):\n",
    "        return [\n",
    "            MRStep(mapper=self.mapper, reducer= self.reducer,jobconf={\n",
    "                    \"mapred.map.tasks\":16,\n",
    "                    \"mapred.reduce.tasks\":8\n",
    "                    }\n",
    "                  ),\n",
    "            MRStep(mapper=self.mapper2 ,combiner=self.combiner2, reducer=self.reducer2,\n",
    "                   jobconf={\n",
    "                    \"mapred.map.tasks\":8,\n",
    "                    \"mapred.reduce.tasks\":4\n",
    "                    }\n",
    "                  )\n",
    "               ]\n",
    "    \n",
    "    def mapper(self, _, line):\n",
    "        total_sqrt = 0\n",
    "        total_sq_cnt= 0 \n",
    "        key,terms = line.strip().split('\\t')\n",
    "        docs = eval(terms)\n",
    "        #normalise the counts for cosine similarity\n",
    "        for word, count in docs.iteritems():\n",
    "            total_sq_cnt += count**2\n",
    "        total_sqrt = math.sqrt(total_sq_cnt)\n",
    "        for doc,count in docs.iteritems():\n",
    "            yield doc,(key.replace('\"', ''), 1.0*count/total_sqrt)\n",
    "    \n",
    "    def reducer(self,key,value):\n",
    "        doc_list ={}\n",
    "        for doc,dist in value:\n",
    "            doc_list[doc]=dist\n",
    "        yield key, doc_list\n",
    "        \n",
    "    def mapper2(self,key,value):\n",
    "        keys = value.keys()\n",
    "        for key1 in keys:\n",
    "            for key2 in keys:\n",
    "                if(key1 == key2):\n",
    "                    continue\n",
    "                multiplied_keys = value[key1]*value[key2]\n",
    "                yield(key1,key2),multiplied_keys\n",
    "    \n",
    "    def combiner2(self,key,value):\n",
    "        yield key,sum(value)\n",
    "    \n",
    "    def reducer2(self,key,value):\n",
    "        yield key,sum(value)\n",
    "    \n",
    "                   \n",
    "if __name__ == '__main__':\n",
    "    Cosine.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!python ./cosine_similarity.py -r hadoop inv_index.out -q >cosine_similarity.out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Got unexpected keyword arguments: ssh_tunnel\n",
      "using configs in /Users/hetal/.mrjob.conf\n",
      "using existing scratch bucket mrjob-f7f5fce3ffcc49a1\n",
      "using s3://mrjob-f7f5fce3ffcc49a1/tmp/ as our scratch dir on S3\n",
      "creating tmp directory /var/folders/91/cjfxt7ys6958qll6vjtgwwfw0000gn/T/cosine_similarity.hetal.20160215.180250.904567\n",
      "writing master bootstrap script to /var/folders/91/cjfxt7ys6958qll6vjtgwwfw0000gn/T/cosine_similarity.hetal.20160215.180250.904567/b.py\n",
      "Copying non-input files into s3://mrjob-f7f5fce3ffcc49a1/tmp/cosine_similarity.hetal.20160215.180250.904567/files/\n",
      "Waiting 5.0s for S3 eventual consistency\n",
      "Creating Elastic MapReduce job flow\n",
      "Job flow created with ID: j-KKUIS43HB212\n",
      "Created new job flow j-KKUIS43HB212\n",
      "Job launched 30.8s ago, status STARTING: Provisioning Amazon EC2 capacity\n",
      "Job launched 62.3s ago, status STARTING: Provisioning Amazon EC2 capacity\n",
      "Job launched 93.1s ago, status STARTING: Provisioning Amazon EC2 capacity\n",
      "Job launched 124.6s ago, status STARTING: Provisioning Amazon EC2 capacity\n",
      "Job launched 155.5s ago, status STARTING: Provisioning Amazon EC2 capacity\n",
      "Job launched 186.7s ago, status STARTING: Configuring cluster software\n",
      "Job launched 217.9s ago, status BOOTSTRAPPING: Running bootstrap actions\n",
      "Job launched 249.2s ago, status RUNNING: Running step (cosine_similarity.hetal.20160215.180250.904567: Step 1 of 2)\n",
      "Job launched 280.0s ago, status RUNNING: Running step (cosine_similarity.hetal.20160215.180250.904567: Step 1 of 2)\n",
      "Job launched 311.5s ago, status RUNNING: Running step (cosine_similarity.hetal.20160215.180250.904567: Step 1 of 2)\n",
      "Job launched 342.4s ago, status RUNNING: Running step (cosine_similarity.hetal.20160215.180250.904567: Step 2 of 2)\n",
      "Job launched 373.4s ago, status RUNNING: Running step (cosine_similarity.hetal.20160215.180250.904567: Step 2 of 2)\n",
      "Job launched 404.2s ago, status RUNNING: Running step (cosine_similarity.hetal.20160215.180250.904567: Step 2 of 2)\n",
      "Job completed.\n",
      "Running time was 152.0s (not counting time spent waiting for the EC2 instances)\n",
      "Fetching counters from S3...\n",
      "Waiting 5.0s for S3 eventual consistency\n",
      "Counters from step 1:\n",
      "  File Input Format Counters :\n",
      "    Bytes Read: 255569\n",
      "  File Output Format Counters :\n",
      "    Bytes Written: 414709\n",
      "  FileSystemCounters:\n",
      "    FILE_BYTES_READ: 364828\n",
      "    FILE_BYTES_WRITTEN: 1539674\n",
      "    HDFS_BYTES_READ: 2000\n",
      "    HDFS_BYTES_WRITTEN: 414709\n",
      "    S3_BYTES_READ: 255569\n",
      "  Job Counters :\n",
      "    Launched map tasks: 20\n",
      "    Launched reduce tasks: 8\n",
      "    Rack-local map tasks: 20\n",
      "    SLOTS_MILLIS_MAPS: 203610\n",
      "    SLOTS_MILLIS_REDUCES: 131378\n",
      "    Total time spent by all maps waiting after reserving slots (ms): 0\n",
      "    Total time spent by all reduces waiting after reserving slots (ms): 0\n",
      "  Map-Reduce Framework:\n",
      "    CPU time spent (ms): 48760\n",
      "    Combine input records: 0\n",
      "    Combine output records: 0\n",
      "    Map input bytes: 208957\n",
      "    Map input records: 991\n",
      "    Map output bytes: 546610\n",
      "    Map output materialized bytes: 424180\n",
      "    Map output records: 12138\n",
      "    Physical memory (bytes) snapshot: 9037938688\n",
      "    Reduce input groups: 991\n",
      "    Reduce input records: 12138\n",
      "    Reduce output records: 991\n",
      "    Reduce shuffle bytes: 424180\n",
      "    SPLIT_RAW_BYTES: 2000\n",
      "    Spilled Records: 24276\n",
      "    Total committed heap usage (bytes): 9037152256\n",
      "    Virtual memory (bytes) snapshot: 37196685312\n",
      "Counters from step 2:\n",
      "  File Input Format Counters :\n",
      "    Bytes Read: 424012\n",
      "  File Output Format Counters :\n",
      "    Bytes Written: 6722741\n",
      "  FileSystemCounters:\n",
      "    FILE_BYTES_READ: 4827367\n",
      "    FILE_BYTES_WRITTEN: 10175129\n",
      "    HDFS_BYTES_READ: 425443\n",
      "    S3_BYTES_WRITTEN: 6722741\n",
      "  Job Counters :\n",
      "    Data-local map tasks: 9\n",
      "    Launched map tasks: 9\n",
      "    Launched reduce tasks: 4\n",
      "    SLOTS_MILLIS_MAPS: 173470\n",
      "    SLOTS_MILLIS_REDUCES: 88968\n",
      "    Total time spent by all maps waiting after reserving slots (ms): 0\n",
      "    Total time spent by all reduces waiting after reserving slots (ms): 0\n",
      "  Map-Reduce Framework:\n",
      "    CPU time spent (ms): 39170\n",
      "    Combine input records: 177900\n",
      "    Combine output records: 173234\n",
      "    Map input bytes: 414709\n",
      "    Map input records: 991\n",
      "    Map output bytes: 8148466\n",
      "    Map output materialized bytes: 4996554\n",
      "    Map output records: 177900\n",
      "    Physical memory (bytes) snapshot: 3504562176\n",
      "    Reduce input groups: 146898\n",
      "    Reduce input records: 173234\n",
      "    Reduce output records: 146898\n",
      "    Reduce shuffle bytes: 4996554\n",
      "    SPLIT_RAW_BYTES: 1431\n",
      "    Spilled Records: 346468\n",
      "    Total committed heap usage (bytes): 3513253888\n",
      "    Virtual memory (bytes) snapshot: 17087664128\n",
      "removing tmp directory /var/folders/91/cjfxt7ys6958qll6vjtgwwfw0000gn/T/cosine_similarity.hetal.20160215.180250.904567\n",
      "Removing all files in s3://mrjob-f7f5fce3ffcc49a1/tmp/cosine_similarity.hetal.20160215.180250.904567/\n",
      "Removing all files in s3://hvcemrbucket/logs/j-KKUIS43HB212/\n",
      "Terminating job flow: j-KKUIS43HB212\n"
     ]
    }
   ],
   "source": [
    "!python ./cosine_similarity.py  \\\n",
    "-r emr s3://hvcemrbucket/output/hw5/stripes/ \\\n",
    "--output-dir=s3://hvcemrbucket/output/hw5/cosine_similarity \\\n",
    "--no-output \\\n",
    "--no-strict-protocol "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### HW 5.5 Evaluation of synonyms that your discovered\n",
    "In this part of the assignment you will evaluate the success of you synonym detector.\n",
    "Take the top 1,000 closest/most similar/correlative pairs of words as determined\n",
    "by your measure in (2), and use the synonyms function in the accompanying\n",
    "python code:\n",
    "\n",
    "nltk_synonyms.py\n",
    "\n",
    "Note: This will require installing the python nltk package:\n",
    "\n",
    "http://www.nltk.org/install.html\n",
    "\n",
    "and downloading its data with nltk.download().\n",
    "\n",
    "For each (word1,word2) pair, check to see if word1 is in the list, \n",
    "synonyms(word2), and vice-versa. If one of the two is a synonym of the other, \n",
    "then consider this pair a 'hit', and then report the precision, recall, and F1 measure  of \n",
    "your detector across your 1,000 best guesses. Report the macro averages of these measures.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "download: s3://hvcemrbucket/output/hw5/jaccard/_SUCCESS to output/jaccard_similarity/_SUCCESS\n",
      "download: s3://hvcemrbucket/output/hw5/jaccard/part-00000 to output/jaccard_similarity/part-00000\n"
     ]
    }
   ],
   "source": [
    "#get top 1000 closest pairs from jaccard similarity\n",
    "!rm -fR output/jaccard_similarity\n",
    "!mkdir -p ./output/jaccard_similarity\n",
    "!aws s3 cp --recursive s3://hvcemrbucket/output/hw5/jaccard ./output/jaccard_similarity\n",
    "!rm ./output/jaccard_similarity_all.out\n",
    "!cat ./output/jaccard_similarity/part* | sort -k3nr  > ./output/jaccard_similarity_all.out\n",
    "!rm -fR output/jaccard_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "download: s3://hvcemrbucket/output/hw5/cosine_similarity/_SUCCESS to output/cosine_similarity/_SUCCESS\n",
      "download: s3://hvcemrbucket/output/hw5/cosine_similarity/part-00002 to output/cosine_similarity/part-00002\n",
      "download: s3://hvcemrbucket/output/hw5/cosine_similarity/part-00001 to output/cosine_similarity/part-00001\n",
      "download: s3://hvcemrbucket/output/hw5/cosine_similarity/part-00003 to output/cosine_similarity/part-00003\n",
      "download: s3://hvcemrbucket/output/hw5/cosine_similarity/part-00000 to output/cosine_similarity/part-00000\n",
      "rm: ./output/cosine_similarity_all.out: No such file or directory\n"
     ]
    }
   ],
   "source": [
    "#get top 1000 closest pairs from cosine similarity\n",
    "# some of the putputs have\n",
    "!rm -fR output/cosine_similarity\n",
    "!mkdir -p ./output/cosine_similarity\n",
    "!aws s3 cp --recursive s3://hvcemrbucket/output/hw5/cosine_similarity ./output/cosine_similarity\n",
    "!rm ./output/cosine_similarity_all.out\n",
    "!cat ./output/cosine_similarity/part* | sort -k3nr  > ./output/cosine_similarity_all.out\n",
    "!rm -fR output/cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of hits:  4\n",
      "Number of misses:  996\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import wordnet as wn\n",
    "import sys\n",
    "\n",
    "jaccard = './output/jaccard_similarity_all.out'\n",
    "cosine = './output/cosine_similarity_all.out'\n",
    "\n",
    "\n",
    "\n",
    "def synonyms(string):\n",
    "    syndict = {}\n",
    "    for i,j in enumerate(wn.synsets(string)):\n",
    "        syns = j.lemma_names()\n",
    "        for syn in syns:\n",
    "            syndict.setdefault(syn,1)\n",
    "    return syndict.keys()\n",
    "\n",
    "\n",
    "def evaluate(string):\n",
    "    syn_data = []\n",
    "    hits = 0\n",
    "    miss = 0\n",
    "    with open(string) as f:\n",
    "        for line in f:\n",
    "            key,terms = line.strip().split('\\t')\n",
    "            docs = eval(key)\n",
    "            syn_data.append((docs[0],docs[1],float(terms)))\n",
    "    # todo : implement topn using heap\n",
    "    sorted_data= sorted(syn_data, key=lambda row: float(row[2]), reverse=True)\n",
    "    sorted_syn = sorted_data[:1000]\n",
    "    \n",
    "    for data in sorted_syn:\n",
    "        if (data[1] in synonyms(data[0])) and (data[0] in synonyms(data[1]) ):\n",
    "            hits += 1\n",
    "        else:\n",
    "            miss += 1\n",
    "    print \"Number of hits: \",hits\n",
    "    print 'Number of misses: ',miss\n",
    "    \n",
    "evaluate(cosine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
